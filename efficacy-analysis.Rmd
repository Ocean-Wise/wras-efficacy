---
title: "WRAS Efficacy Analysis"
author: "Alex Mitchell"
date: "2023-05-15"
output: 
  rmdformats::downcute:
    downcute_theme: "chaos"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#### No QA has been done. 

# Introduction

The Whale Report Alert System, running since 2018, has not been empirically tested to assess the effectiveness of the tool. In order to assess our own tool, push for further funding, and show the effectiveness to international partners we need to have evidence of its success.

Therefore the aims of the project are to...
* Use survey data to understand more about mariners behavior when using the WRAS from their perspective
* Use AIS and WRAS alert data to test how often a vessel changes speed or direction due to a WRAS alert

## Data
### Ocean Wise
* WRAS Alert data
  + This comes from an extract of the database from Lapis. It contains information on all users who received an alert from a sighting.
  + This does not include operations center staff/desktop application users.

* WRAS User data
  + Some users should not be included in the analysis, for example Ocean Wise staff. We can use this to filter out users we want to ignore.

* Vessel data
  + Contains information on all the vessels logged through the WRAS alerts data.
  + Very messy, containing a lot of user error in entering data.
  + I have spent hours on MarineTraffic identifying vessels and additional information.
  + We need to join the vessel data with the WRAS data as we will use the MMSI number of the vessel to enable us to use WRAS alert data with the AIS data.
  
### Vessel position data
* AIS
  + Extract from Ocean Networks Canada (ONC - Kiyomi Holman).
  + Already pre-processed by ONC.
  + Columns: UTC Time | Local | Time | MMSI | IMO |	Name | Type |	Length |	Beam | Draught | SOG | COG | Heading |	Latitude |	Longitude |	Destination |	Status |	Flag
    + We will use the SOG (speed over ground) and status to determine speed changes, and lat/lng to determine change in direction.



### Survey data
* Microsoft form sent out by Ocean Wise to WRAS users.
  + TBD
  
# Data Cleaning

```{r warning = F, message = F}
library(magrittr)
library(lubridate) ## We need to load this specifically for the %within% function

## Edit this if you're running on your own Ocean Wise machine. 
user = "AlexMitchell"
```

In this section, we will perform data cleaning tasks to prepare the dataset for analysis. Describe the steps taken to clean the data, such as handling missing values, removing duplicates, correcting inconsistent values, or transforming variables.

## Load Data

### WRAS Alert Data

```{r  warning = F, message = F}
alerts = readxl::read_xlsx(paste0("C:/Users/",
                                             user,
                                             "/Ocean Wise Conservation Association/Whales Initiative - General/OceanWise Data/WRAS/WRAS_main.xlsx")) %>% 
  janitor::clean_names()
```

### WRAS Users

```{r warning = F, message = F}
users = readxl::read_xlsx(paste0("C:/Users/",
                                       user,
                                       "/Ocean Wise Conservation Association/Whales Initiative - General/BCCSN.Groups/WhaleReport Alert System/Participants/WRASUSERS_main.xlsx"), sheet = "Authorized") %>% 
  janitor::clean_names()
```

### Vessel data

```{r warning = F, message = F}
vessels = readxl::read_xlsx(paste0("C:/Users/",
                                             user,
                                             "/Ocean Wise Conservation Association/Whales Initiative - General/OceanWise Data/WRAS/Vessels_main.xlsx")) %>% 
  janitor::clean_names()

```

### AIS

Given how large this data is (~ 6-7GB per folder), and that it is split into weekly files, we will have to write a loop to go over the data, clean it and process it at the same time. The data will be loaded and processed later on after creating a cleaned alerts dataset.


## Data Cleaning Tasks

Perform specific data cleaning tasks, such as handling missing values, removing duplicates, correcting inconsistent values, or transforming variables. Provide code examples and explanations for each task.

### Users

* Relatively simple cleaning by removing special characters from all columns barring emails.
* The `include_in_efficacy_analysis` filter removes Ocean Wise staff from the analysis.

```{r warning = F, message = F}
users_clean = users %>% 
  dplyr::mutate(dplyr::across(!email, fedmatch::clean_strings)) %>%  ## cleans strings to remove special characters and formatting (all cols but email)
  dplyr::mutate(name = paste(first_name, last_name)) %>% 
  dplyr::select(name, organization, org_type, email, region_clean, include_in_efficacy_analysis)
  
```


### Vessels
 
* Splitting the `loa` and `be` column to create a length and breadth columns and extracting the numbers from those columns to remove odd characters.
* I added a `Key2` column in here which is just a row number, this is needed for the `fuzzy_matching` later on.
* Removing the "mv" and "m v" parts of strings as they appear 

```{r warning = F, message = F}
vessels_clean = vessels %>%
  dplyr::mutate(dplyr::across(!loa_x_be, fedmatch::clean_strings)) %>%
  tidyr::separate(loa_x_be, c("loa", "be"),sep = "x") %>%  ## split loa x be column to two seperate columns
  dplyr::mutate(dplyr::across(c(loa, be), readr::parse_number)) %>%  ## extract numerics from the new columns
  dplyr::mutate(key2 = 1:nrow(.)) %>% 
  dplyr::mutate(vessel_name = stringr::str_remove_all(vessel_name, "mv |m v ")) %>% 
  dplyr::select(vessel_name, mmsi, imo, class, loa, be, notes, key2)
```


### Alerts

* Clean strings to remove special characters
* Filter out the Ocean Wise vessels
* Again, create a key for the fuzzy matching that happens shortly
* Remove acronyms before vessel names due to them only partially being used
* We need to adjust some of the names of vessels as the fuzzy matching can't handle the miss-spellings. This is a manual process and will need to be updated when we get more WRAS alerts. 

```{r warning = F, message = F}
alerts_clean = alerts %>% 
  dplyr::select(c("alert_sent",
                  # "sighted_at",
                  "latitude", 
                  "longitude", 
                  "skipper", 
                  "vessel", 
                  "species")) %>% 
  dplyr::mutate(dplyr::across(!longitude & !latitude & !alert_sent, fedmatch::clean_strings), ## cleans strings to remove special characters and formatting
                alert_sent = lubridate::force_tz(alert_sent, "Canada/Pacific")) %>%  ## I had to force the time zone for the later interval to be in the correct time zone as well. 
  dplyr::filter(!vessel %in% c("skana", "kellahan","tsitika","no vessel", 
                               "land",
                               "on land",
                               "pilot underway")) %>%
  ## Assigns a unique key to the data set for the fuzzy matching later
  dplyr::mutate(key1 = 1:nrow(.)) %>% 
  dplyr::mutate(vessel = stringr::str_remove_all(vessel, "mv |m v ")) %>% 
  ## The next mutates clean up some of the common spelling errors and bits the fuzzy matching misses. Repeated for vessel data.
  dplyr::mutate(vessel = dplyr::case_when(vessel == "charles hays amwaal" ~ "charles hays",
                                          vessel == "amwaal" ~ "charles hays",
                                          vessel == "cowichan" ~ "queen of cowichan",
                                          vessel == "sobc" ~ "spirit of british columbia",
                                          vessel == "qalb" ~ "queen of alberni",
                                          vessel == "spirit of bc" ~ "spirit of british columbia",
                                          vessel == "reliant" ~ "seaspan reliant",
                                          vessel == "queen of newwest" ~ "queen of new westminster",
                                          vessel == "q of alberni" ~ "queen of alberni",
                                          vessel == "mazuru bishamon" ~ "maizuru bishamon",
                                          vessel == "coastal renn" ~ "coastal renaissance",
                                          vessel == "sovc" ~ "spirit of vancouver island",
                                          vessel == "q of alberni" ~ "queen of alberni",
                                          vessel == "qnw" ~ "queen of new westminster",
                                          vessel == "laurier" ~ "sir wilfrid laurier",
                                          vessel == "suquamish" ~ "wsf suquamish",
                                          vessel == "howe sounbd queen" ~ "howe sound queen",
                                          vessel == "inspiration" ~ "coastal inspiration",
                                          vessel == "suquwamish" ~ "wsf suquamish",
                                          vessel == "seapan zambizi" ~ "seaspan zambezi",
                                          vessel == "sprit of british columbia" ~ "spirit of british columbia",
                                          vessel == "roald almundsen" ~ "roald amundsen",
                                          vessel == "ovean clio" ~ "ocean clio",
                                          vessel == "coroleader ol" ~ "coreleader ol",
                                          vessel == "zuidetdam" ~ "zuiderdam",
                                          vessel == "seaspan anadonis" ~ "seaspan adonis",
                                          vessel == "berge yotie" ~ "berge yotei",
                                          vessel == "carnval splendor" ~ "carnival splendor",
                                          vessel == "blackball" ~ "coho",
                                          vessel == "zeta" ~ "star zeta",
                                          vessel == "cma cgm rigalito" ~ "cma cgm rigoletto",
                                          vessel == "gulf islands spirit" ~ "spirit of vancouver island",
                                          vessel == "zeda" ~ "star zeta",
                                          vessel == "zeta" ~ "star zeta",
                                          TRUE ~ vessel)) %>% 
  dplyr::mutate(dplyr::across(!c(latitude, longitude, alert_sent),fedmatch::clean_strings)) 


```


# Data Processing

In this section, we will process the cleaned data to extract relevant information, create new variables, or derive insights. Describe the steps involved in data processing and any transformations or calculations performed.

## Data Transformation

### Matching WRAS and Vessel data

WRAS alert data does not contain MMSI number of vessel which we need to use to identify vessel tracks in AIS. WRAS alert data contains spelling mistakes in the vessel name due to human error. We need a way to match the WRAS alert data to the vessel data, and we can only do that by name. Fuzzy matching will allow us to account for human error in spelling in data imports from WRAS data to match vessel names in our main vessel dataset. These settings have been changed a little bit, with the maxDist set at 0.5 we were getting incorrect matches The "checkMatch" object contains a df of vessels names which don't exactly match. I manually checked through this. Dataframe and manually renamed vessel names in the WRAS_data_import. With this many vessel names some are to close (in string distance) for the function to accurately match correctly in some instances.


```{r warning = F, message = F}
WRAS_data_matched <- fedmatch::merge_plus(data1 = alerts_clean, 
                                          match_type = "fuzzy",
                                          data2 = vessels_clean,
                                          by.x = "vessel", by.y = "vessel_name",
                                          unique_key_1 = "key1", unique_key_2 = "key2", 
                                          suffixes = c("_1", "_2"),
                                          fuzzy_settings = fedmatch::build_fuzzy_settings(maxDist = 0.1))


## List of vessels without MMSI numbers - uncomment to view the dataset
# View(tibble::as_tibble(unique(WRAS_data_matched$data1_nomatch$vessel)))

## Cleaned data
WRAS_data_midclean <- WRAS_data_matched$matches


```

The final steps before we have cleaned alert data ready to process the AIS data - 
* we need to filter for users from Ocean Wise and other non-mariner organizations who should not be included in the efficacy analysis - they would likely not take action to a WRAS alert. 
* create a time interval of before and after an alert was sent to compare during analysis. This will also be used to reduce the size of the AIS data via a join later on. 

```{r warning = F, message = F}
user_filter = users_clean$name[users_clean$include_in_efficacy_analysis == "no"]

WRAS_data_cleaned = WRAS_data_midclean %>% 
  dplyr::filter(!skipper %in% user_filter) %>% 
  dplyr::select(-c(key1, key2, notes, tier)) %>% 
  dplyr::mutate(mmsi = as.numeric(mmsi)) %>% 
  dplyr::mutate(range = lubridate::interval(alert_sent - lubridate::minutes(30), # create an interval 30mins before and after an alert was sent. 
                                            alert_sent + lubridate::minutes(30)),
                date = lubridate::as_date(alert_sent)) 

## Remove uneccesary objects from environment to reduce storage in memory.
rm(WRAS_data_matched, WRAS_data_midclean)


```

## AIS data

The AIS data is already in a clean format, but we need to process it. Processing steps:
* filter to remove ships which are: at anchor; moored; not under control; restricted maneuverability; pushing or towing alongside, or engaged in fishing. 
  + This [link](https://datalastic.com/blog/ais-navigational-status/) describes the catagories of *status* well.
* Then need to join data if the MSSI matches, the date matches, a the time is -30 or +30 mins from the time of the alert.

This should build up a large dataset of alert info + vessel information

```{r warning = F, message = F}

status_filter = list("Moored", "Not under command", "Engaged in fishing", "At anchor", "Pushing or towing alongside", "Restricted manoeuvrability")

## Create a list of folder paths to loop a function over to get all individual file names.
folder_paths = c(paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/OceanWise Data/AIS/2019"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/OceanWise Data/AIS/2020"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/OceanWise Data/AIS/2021"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/OceanWise Data/AIS/2022"))


## Function to get all file names in folders
get_file_names = function(folder_path){
  files = list.files(path = folder_path, full.names = T)
}

## Create a list of all file names I want to iterate a function over
file_list = purrr::map(folder_paths, get_file_names) %>% 
  unlist()
  

## Loop function to load AIS data, clean it, mutate columns to correct format then join to the WRAS alerts data.

loop_function = function(file_path) {
  ## Going to use data.table::fread to load as it is exponentially faster than read.csv.
  ais_data = data.table::fread(file_path) %>% 
    janitor::clean_names() %>% 
    dplyr::filter(!status %in% status_filter) %>% 
    dplyr::mutate(local_time = lubridate::force_tz(lubridate::as_datetime(local_time), tzone = "Canada/Pacific"),
                  date = lubridate::date(local_time))
  



  matched_alerts = ais_data %>% 
    dplyr::inner_join(WRAS_data_cleaned, by = c("date", "mmsi")) %>% 
    dplyr::mutate(match = dplyr::case_when(local_time %within% range ~ "Y",
                                           TRUE ~ "N")) %>% 
    dplyr::filter(match == "Y") ## If the AIS data falls within the alert interval we set earlier (and labelled it above with "Y" then keep these rows). 

}

```


No that we have our have created a list of files from within the folders of AIS data between 2019 - 2022 we can loop over these file names and join to our WRAS data creating our final dataset.


```{r warning = F, message = F}
final_data = purrr::map(file_list, loop_function) %>% 
  purrr::discard(~ nrow(.x) == 0) %>% 
  purrr::reduce(dplyr::full_join)

```


# Analysis

## Exploration

### Speed over ground

I want to see how SOG is distributed throughtout our data, and if there are any values we can trim out, for instance extrememly low speeds (< 3knots maybe) which would indicate a vessel arriving at port, or any high speeds which may be an AIS malfunction.

```{r}

sog_explore = final_data %>% 
  ggplot2::ggplot(data = .) +
  ggplot2::geom_histogram(mapping = ggplot2::aes(x = sog), binwidth = 0.2)

sog_explore

```

Spike at < 5knots, and some additional noise over 25 knots. I want to know more about the distribution in these areas, what vessels are travelling at such low speeds and plot out where these low speeds tend to be (I have a feeling it will be in port), and then what vessels are travelling at high speeds (I think thiese could be enforcement or anomolys but lets test).



```{r}
low_speed = final_data %>%
  ggplot2::ggplot(.) + 
  ggplot2::geom_histogram(mapping = ggplot2::aes(x = sog), binwidth = 0.1) +
  ggplot2::coord_cartesian(xlim = c(0, 10))

low_speed

```

Large cluster approaching 0. Why? I imagine that the majority of these will be within port boundries, or near mooring bouys/tug areas. I can make a map to test this.

```{r}

low_speed_map = final_data %>% 
  dplyr::filter(sog < 2)  

leaflet::leaflet() %>% 
  leaflet::addTiles() %>% 
  leaflet::addCircleMarkers(lng = low_speed_map$longitude.x, lat = low_speed_map$latitude.x)


```

```{r, message=FALSE}
less_4 = final_data %>% 
  dplyr::filter(sog < 4)  

less_2 = final_data %>% 
  dplyr::filter(sog < 2)  

diff_in_datasets = nrow(less_4)-nrow(less_2) 
diff_in_datasets
```

Looks like there are vessels outside of ports and harbors which may slow to speeds of less than 4 knots, if I take it down to less than 2 knots we have `diff_in_datasets` less records totaling `nrow(less_2)`, nearly 10% of the data.

I wonder if I was to look at when the vessel speed is 0, if it is before the alert is sent then I remove that record... as we can likely assume that the vessel was in harbor or moving slowly out of the harbor when the alert was sent. 


---

