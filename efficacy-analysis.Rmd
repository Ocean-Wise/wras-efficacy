---
title: "WRAS Efficacy Analysis"
author: "Alex Mitchell"
date: "2023-05-15"
output: 
  rmdformats::downcute:
    downcute_theme: "chaos"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### No QA has been done.

# Introduction

The Whale Report Alert System, running since 2018, has not been empirically tested to assess the effectiveness of the tool. In order to assess our own tool, push for further funding, and show the effectiveness to international partners we need to have evidence of its success.

Therefore the aims of the project are to...

-   Use survey data to understand more about mariners behavior when using the WRAS from their perspective

-   Use AIS and WRAS alert data to test how often a vessel changes speed or direction due to a WRAS alert

## Data

### Ocean Wise

-   WRAS Alert data
    -   This comes from an extract of the database from Lapis. It contains information on all users who received an alert from a sighting.
    -   This does not include operations center staff/desktop application users.
-   WRAS User data
    -   Some users should not be included in the analysis, for example Ocean Wise staff. We can use this to filter out users we want to ignore.
-   Vessel data
    -   Contains information on all the vessels logged through the WRAS alerts data.
    -   Very messy, containing a lot of user error in entering data.
    -   I have spent hours on MarineTraffic identifying vessels and additional information.
    -   We need to join the vessel data with the WRAS data as we will use the MMSI number of the vessel to enable us to use WRAS alert data with the AIS data.

### Vessel position data

-   AIS
    -   Extract from Ocean Networks Canada (ONC - Kiyomi Holman).
    -   Already pre-processed by ONC.
    -   Columns: UTC Time \| Local \| Time \| MMSI \| IMO \| Name \| Type \| Length \| Beam \| Draught \| SOG \| COG \| Heading \| Latitude \| Longitude \| Destination \| Status \| Flag
        -   We will use the SOG (speed over ground) and status to determine speed changes, and lat/lng to determine change in direction.

### Survey data

-   Microsoft form sent out by Ocean Wise to WRAS users.
    -   TBD

# Data Cleaning

```{r warning = F, message = F}
library(magrittr)
library(lubridate) ## We need to load this specifically for the %within% function

## Edit this if you're running on your own Ocean Wise machine. 
user = "AlexMitchell"
```

In this section, we will perform data cleaning tasks to prepare the dataset for analysis. Describe the steps taken to clean the data, such as handling missing values, removing duplicates, correcting inconsistent values, or transforming variables.

## Load Data

### WRAS Alert Data

```{r  warning = F, message = F}
alerts = readxl::read_xlsx(paste0("C:/Users/",
                                             user,
                                             "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/WRAS/WRAS_main.xlsx")) %>% 
  janitor::clean_names()
```

### WRAS Users

```{r warning = F, message = F}
users = readxl::read_xlsx(paste0("C:/Users/",
                                       user,
                                       "/Ocean Wise Conservation Association/Whales Initiative - General/BCCSN.Groups/WhaleReport Alert System/Participants/WRASUSERS_main.xlsx"), sheet = "Authorized") %>% 
  janitor::clean_names()
```

### Vessel data

```{r warning = F, message = F}
vessels = readxl::read_xlsx(paste0("C:/Users/",
                                             user,
                                             "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/WRAS/Vessels_main.xlsx")) %>% 
  janitor::clean_names()

```

### AIS

Given how large this data is (\~ 6-7GB per folder), and that it is split into weekly files, we will have to write a loop to go over the data, clean it and process it at the same time. The data will be loaded and processed later on after creating a cleaned alerts dataset.

## Data Cleaning Tasks

Perform specific data cleaning tasks, such as handling missing values, removing duplicates, correcting inconsistent values, or transforming variables. Provide code examples and explanations for each task.

### Users

-   Relatively simple cleaning by removing special characters from all columns barring emails.

```{r warning = F, message = F}
users_clean = users %>% 
  dplyr::mutate(dplyr::across(!email, fedmatch::clean_strings)) %>%  ## cleans strings to remove special characters and formatting (all cols but email)
  dplyr::mutate(name = paste(first_name, last_name)) %>% 
  dplyr::select(name, organization, org_type, email, region_clean, include_in_efficacy_analysis)
  
```

### Vessels

-   Splitting the `loa` and `be` column to create a length and breadth columns and extracting the numbers from those columns to remove odd characters.
-   I added a `Key2` column in here which is just a row number, this is needed for the `fuzzy_matching` later on.
-   Removing the "mv" and "m v" parts of strings as they appear

```{r warning = F, message = F}
vessels_clean = vessels %>%
  dplyr::mutate(dplyr::across(!loa_x_be, fedmatch::clean_strings)) %>%
  tidyr::separate(loa_x_be, c("loa", "be"),sep = "x") %>%  ## split loa x be column to two seperate columns
  dplyr::mutate(dplyr::across(c(loa, be), readr::parse_number)) %>%  ## extract numerics from the new columns
  dplyr::mutate(key2 = 1:nrow(.)) %>% 
  dplyr::mutate(vessel_name = stringr::str_remove_all(vessel_name, "mv |m v ")) %>% 
  dplyr::select(vessel_name, mmsi, imo, class, loa, be, notes, key2)
```

### Alerts

-   Clean strings to remove special characters
-   Filter out the Ocean Wise vessels
-   Again, create a key for the fuzzy matching that happens shortly
-   Remove acronyms before vessel names due to them only partially being used
-   We need to adjust some of the names of vessels as the fuzzy matching can't handle the miss-spellings. This is a manual process and will need to be updated when we get more WRAS alerts.

```{r warning = F, message = F}
alerts_clean = alerts %>% 
  dplyr::select(c("alert_sent",
                  # "sighted_at",
                  "latitude", 
                  "longitude", 
                  "skipper", 
                  "vessel", 
                  "species")) %>% 
  dplyr::mutate(dplyr::across(!longitude & !latitude & !alert_sent, fedmatch::clean_strings), ## cleans strings to remove special characters and formatting
                alert_sent = lubridate::force_tz(alert_sent, "Canada/Pacific")) %>%  ## I had to force the time zone for the later interval to be in the correct time zone as well. 
  dplyr::filter(!vessel %in% c("skana", "kellahan","tsitika","no vessel", 
                               "land",
                               "on land",
                               "pilot underway")) %>%
  ## Assigns a unique key to the data set for the fuzzy matching later
  dplyr::mutate(key1 = 1:nrow(.)) %>% 
  dplyr::mutate(vessel = stringr::str_remove_all(vessel, "mv |m v ")) %>% 
  ## The next mutates clean up some of the common spelling errors and bits the fuzzy matching misses. Repeated for vessel data.
  dplyr::mutate(vessel = dplyr::case_when(vessel == "charles hays amwaal" ~ "charles hays",
                                          vessel == "amwaal" ~ "charles hays",
                                          vessel == "cowichan" ~ "queen of cowichan",
                                          vessel == "sobc" ~ "spirit of british columbia",
                                          vessel == "qalb" ~ "queen of alberni",
                                          vessel == "spirit of bc" ~ "spirit of british columbia",
                                          vessel == "reliant" ~ "seaspan reliant",
                                          vessel == "queen of newwest" ~ "queen of new westminster",
                                          vessel == "q of alberni" ~ "queen of alberni",
                                          vessel == "mazuru bishamon" ~ "maizuru bishamon",
                                          vessel == "coastal renn" ~ "coastal renaissance",
                                          vessel == "sovc" ~ "spirit of vancouver island",
                                          vessel == "q of alberni" ~ "queen of alberni",
                                          vessel == "qnw" ~ "queen of new westminster",
                                          vessel == "laurier" ~ "sir wilfrid laurier",
                                          vessel == "suquamish" ~ "wsf suquamish",
                                          vessel == "howe sounbd queen" ~ "howe sound queen",
                                          vessel == "inspiration" ~ "coastal inspiration",
                                          vessel == "suquwamish" ~ "wsf suquamish",
                                          vessel == "seapan zambizi" ~ "seaspan zambezi",
                                          vessel == "sprit of british columbia" ~ "spirit of british columbia",
                                          vessel == "roald almundsen" ~ "roald amundsen",
                                          vessel == "ovean clio" ~ "ocean clio",
                                          vessel == "coroleader ol" ~ "coreleader ol",
                                          vessel == "zuidetdam" ~ "zuiderdam",
                                          vessel == "seaspan anadonis" ~ "seaspan adonis",
                                          vessel == "berge yotie" ~ "berge yotei",
                                          vessel == "carnval splendor" ~ "carnival splendor",
                                          vessel == "blackball" ~ "coho",
                                          vessel == "zeta" ~ "star zeta",
                                          vessel == "cma cgm rigalito" ~ "cma cgm rigoletto",
                                          vessel == "gulf islands spirit" ~ "spirit of vancouver island",
                                          vessel == "zeda" ~ "star zeta",
                                          vessel == "zeta" ~ "star zeta",
                                          TRUE ~ vessel)) %>% 
  dplyr::mutate(dplyr::across(!c(latitude, longitude, alert_sent),fedmatch::clean_strings)) 


```

# Data Processing

In this section, we will process the cleaned data to extract relevant information, create new variables, or derive insights. Describe the steps involved in data processing and any transformations or calculations performed.

## Data Transformation

### Matching WRAS and Vessel data

WRAS alert data does not contain MMSI number of vessel which we need to use to identify vessel tracks in AIS. WRAS alert data contains spelling mistakes in the vessel name due to human error. We need a way to match the WRAS alert data to the vessel data, and we can only do that by name. Fuzzy matching will allow us to account for human error in spelling in WRAS data to match vessel names in our main vessel dataset. These settings have been changed a little bit, with the maxDist set at 0.5 we were getting incorrect matches The "checkMatch" object contains a df of vessels names which don't exactly match. I manually checked through this. Dataframe and manually renamed vessel names in the WRAS_data_import. With this many vessel names some are too close (in string distance) for the function to accurately match correctly in all instances.

```{r warning = F, message = F}
WRAS_data_matched <- fedmatch::merge_plus(data1 = alerts_clean, 
                                          match_type = "fuzzy",
                                          data2 = vessels_clean,
                                          by.x = "vessel", by.y = "vessel_name",
                                          unique_key_1 = "key1", unique_key_2 = "key2", 
                                          suffixes = c("_1", "_2"),
                                          fuzzy_settings = fedmatch::build_fuzzy_settings(maxDist = 0.1))


## List of vessels without MMSI numbers - uncomment to view the dataset
# View(tibble::as_tibble(unique(WRAS_data_matched$data1_nomatch$vessel)))

## Cleaned data
WRAS_data_midclean <- WRAS_data_matched$matches


```

The final steps before we have cleaned alert data ready to process the AIS data -

-   Filter for users from Ocean Wise and other non-mariner organizations who should not be included in the efficacy analysis - they would likely not take action to a WRAS alert.

-   Create a time interval of before and after an alert was sent to compare during analysis. This will also be used to reduce the size of the AIS data via a join later on.

    -   Imagining a scenario where a vessel is on the edge of the alert radius, the epicenter being 5nm away (total diameter of an alert is 10nm). In 20mins a vessel, travelling at 13.5knots, will reach the alert centre.

    -   For this reason, I have chose 20min as a good guess for how long it would take for a vessel to slow down and stay slowed, any longer we might see and uptick in speed.

-   The `include_in_efficacy_analysis` filter removes Ocean Wise staff from the analysis.

```{r warning = F, message = F}
user_filter = users_clean$name[users_clean$include_in_efficacy_analysis == "no"]

WRAS_data_cleaned = WRAS_data_midclean %>% 
  dplyr::filter(!skipper %in% user_filter) %>% 
  dplyr::select(-c(key1, key2, notes, tier)) %>% 
  dplyr::mutate(mmsi = as.numeric(mmsi)) %>% 
  dplyr::mutate(range = lubridate::interval(alert_sent,
                                            # alert_sent - lubridate::minutes(30), # create an interval 30mins before and after an alert was sent. 
                                            alert_sent + lubridate::minutes(20)),
                date = lubridate::as_date(alert_sent)) 

## Remove uneccesary objects from environment to reduce storage in memory.
rm(WRAS_data_matched, WRAS_data_midclean)


```

## AIS data

The AIS data is already in a clean format, but we need to process it to remove ships which are a particular status (this [link](https://datalastic.com/blog/ais-navigational-status/){style="font-size: 11pt;"} describes the categories of AIS *status*), as well as create a interval before and after alerts to reduce the data size to only relevant data. Processing steps:

-   Filter to remove ships which are:

    -   At anchor;

    -   Moored;

    -   Not under control;

    -   Restricted maneuverability;

    -   Pushing or towing alongside

    -   Engaged in fishing

-   Join data if the MSSI matches, the date matches, a the time is -30 or +30 mins from the time of the alert.

Aim of this cleaning is to reduce the data size drastically to only relevant AIS tracks and vessels which are moving without assistance or hindrance.

```{r warning = F, message = F}

status_filter = list("Moored", "Not under command", "Engaged in fishing", "At anchor", "Pushing or towing alongside", "Restricted manoeuvrability")

## Create a list of folder paths to loop a function over to get all individual file names.
folder_paths = c(paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2019"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2020"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2021"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2022"))


## Function to get all file names in folders
get_file_names = function(folder_path){
  files = list.files(path = folder_path, full.names = T)
}

## Create a list of all file names I want to iterate a function over
file_list = purrr::map(folder_paths, get_file_names) %>% 
  unlist()
  

## Loop function to load AIS data, clean it, mutate columns to correct format then join to the WRAS alerts data.

loop_function = function(file_path) {
  ## Going to use data.table::fread to load as it is exponentially faster than read.csv.
  ais_data = data.table::fread(file_path) %>% 
    janitor::clean_names() %>% 
    dplyr::filter(!status %in% status_filter) %>%
    dplyr::mutate(local_time = lubridate::force_tz(lubridate::as_datetime(local_time), tzone = "Canada/Pacific"),
                  date = lubridate::date(local_time))
  

  matched_alerts = ais_data %>% 
    dplyr::inner_join(WRAS_data_cleaned, by = c("date", "mmsi")) %>% 
    dplyr::mutate(match = dplyr::case_when(local_time %within% range ~ "Y",
                                           TRUE ~ "N")) %>% 
    dplyr::filter(match == "Y") ## If the AIS data falls within the alert interval we set earlier (and labelled it above with "Y" then keep these rows).

}

```

No that we have our have created a list of files from within the folders of AIS data between 2019 - 2022 we can loop over these file names and join to our WRAS data creating our final dataset.

```{r warning = F, message = F}
matched_wras_ais = purrr::map(file_list, loop_function) %>%
  purrr::discard(~ nrow(.x) == 0) %>% 
  purrr::reduce(dplyr::full_join) 

```

## Baseline data

Need to get some baseline data to compare our post-WRAS alert vessel behavior to the average behavior of a vessel. To do this I am going to use the 2019 data given that shipping would've been at pre-pandemic levels, and WRAS engagement would've been relatively (to now) low being a new tool.

```{r}
## function to loop through 2019 data and do some cleaning

baseline_files = list.files(paste0("C:/Users/", user,"/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2019"), full.names = T)


baseline_loop_function = function(file_path) {
  baseline_data = data.table::fread(file_path) %>% 
    janitor::clean_names() %>% 
    dplyr::filter(!status %in% status_filter) %>%
    dplyr::mutate(local_time = lubridate::force_tz(lubridate::as_datetime(local_time), tzone = "Canada/Pacific"),
                  date = lubridate::date(local_time)) %>% 
    dplyr::select(c("date", "mmsi", "name", "type", "length", "beam", "sog", "cog", "heading", "latitude", "longitude"))

  ## This removes vessels that have recieved alerts...
  remove_alerts = baseline_data %>% 
    dplyr::anti_join(WRAS_data_cleaned, by = c("date", "mmsi")) %>%
    dplyr::mutate(type = dplyr::case_when(stringr::str_detect(type, "Tanker") ~ "Tanker",
                                          stringr::str_detect(type, "Cargo ship") ~ "Cargo ship",
                                          stringr::str_detect(type, "Other") ~ "Other",
                                          TRUE ~ type)) %>% 
    dplyr::filter(type %in% c("Cargo ship", "HSC", "Passenger ship"))
}

```

We will now loop over the files in the 2019 folder, filter out any vessels which received WRAS alerts in this time.

```{r}
baseline_data = purrr::map(baseline_files, baseline_loop_function) %>%
  purrr::reduce(dplyr::full_join, by = c("date", "mmsi", "name", "type", "length", "beam", "sog", "cog", "heading", "latitude", "longitude")) %>% 
  dplyr::filter(lubridate::year(date) != 2018) %>% 
  dplyr::filter(sog > 7 & sog < 40)




```

## Final Processing from leanings in the "Explore" section

1.  Filter data speed to reflect realistic expectations of vessel traffic speed, as well as to speeds at which vessels would potentially take action on an alert (7 - 40 knots).
2.  Remove any category of vessel with \< 100 record to create a large sample size.

```{r}

clean_wras_ais_filtered = matched_wras_ais %>%
  dplyr::filter(sog > 7 & sog < 40) %>%
  dplyr::mutate(type = dplyr::case_when(stringr::str_detect(type, "Tanker") ~ "Tanker",
                                        stringr::str_detect(type, "Cargo ship") ~ "Cargo ship",
                                        stringr::str_detect(type, "Other") ~ "Other",
                                        TRUE ~ type))

data_count = clean_wras_ais_filtered %>% 
  dplyr::group_by(type) %>% 
  dplyr::summarise(count = dplyr::n_distinct(alert_sent))

## We have "search and rescue" in the list of vessel types to include in the analysis, I think these should likely be discarded as they don't often have set routes or speeds. 


vessel_type_filter = data_count %>% dplyr::filter(count > 100 & type != "Search/rescue") %>% .$type

for_analysis = clean_wras_ais_filtered %>% 
  dplyr::filter(type %in% vessel_type_filter)




```

# Analyses

## Speed

```{r}


median_baseline = baseline_data %>%
  dplyr::group_by(type) %>%
  dplyr::summarise(median_baseline = median(sog))

median_wras_data = for_analysis %>%
  dplyr::group_by(type) %>%
  dplyr::summarise(median_test = median(sog))

for_analysis %>%
  dplyr::group_by(type) %>%
  ggplot2::ggplot(ggplot2::aes(x = sog)) +
  ggplot2::geom_histogram(binwidth = 1) +
  ggplot2::facet_wrap(~type, scales = "free") +
  ggplot2::labs(x = "SOG", y = "Frequency") +
  ggplot2::theme_minimal()

wras_vs_baseline = for_analysis %>% 
  dplyr::left_join(median_baseline, by = "type") %>% 
  dplyr::left_join(median_wras_data, by = "type") %>% 
  dplyr::group_by(type) %>%  
  ggplot2::ggplot(ggplot2::aes(x = sog)) +
  ggplot2::geom_histogram(binwidth = 1) +
  ggplot2::geom_vline(ggplot2::aes(xintercept = median_baseline), color = "red", linetype = "dashed") +
  ggplot2::geom_vline(ggplot2::aes(xintercept = median_test), color = "darkgrey", linetype = "dashed") +
  ggplot2::facet_wrap(~type, scales = "free") +
  ggplot2::labs(x = "SOG", y = "Frequency") +
  ggplot2::theme_minimal()

baseline_vs_wras = baseline_data %>% 
  dplyr::left_join(median_baseline, by = "type") %>% 
  dplyr::left_join(median_wras_data, by = "type") %>% 
  dplyr::group_by(type) %>%  
  ggplot2::ggplot(ggplot2::aes(x = sog)) +
  ggplot2::geom_histogram(binwidth = 1) +
  ggplot2::geom_vline(ggplot2::aes(xintercept = median_baseline), color = "red", linetype = "dashed") +
  ggplot2::geom_vline(ggplot2::aes(xintercept = median_test), color = "darkgrey", linetype = "dashed") +
  ggplot2::facet_wrap(~type, scales = "free") +
  ggplot2::labs(x = "SOG", y = "Frequency") +
  ggplot2::theme_minimal()

baseline_vs_wras

```

### Results from the general speed change scenario

-   From a high level view of the data, there is actually an increase in speed shown in all three groups. This likely comes from a smaller sample size, and perhaps other factors.

#### What am I thinking about?

-   Picking specific ferry routes or cargo ship routes

    -   where are there slowdowns?

    -   What speed, on average, are vessels travelling anyway in the region?

        -   whats the distribution of speed?

        -   maybe overall there is no change, but on the macro scale there is...

    -   What area does the baseline data cover? I should probably try and re-baseline to only take vessels from the same area. Coastline is complex, it might be that WRAS alerts are received in less remote, faster water areas... I have AIS for remote and perhaps more precarious areas.

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

# Exploration

We need to further explore the data to see whether we can eek out any other necessary cleaning of outlines, or identify some questions to answer.

For the efficacy analysis we are interested in testing whether or not we see a change in speed of direction which could be attributed to an event (WRAS alert). Speed is relatively straight forward, direction is not.

## Speed over ground

I want to see how SOG is distributed through out our data, and if there are any values we can trim out, for instance extremely low speeds (\< 3knots maybe) which would indicate a vessel arriving at port, or any high speeds which may be an AIS malfunction.

```{r}

# sog_explore = matched_wras_ais %>% 
#   ggplot2::ggplot(data = .) +
#   ggplot2::geom_histogram(mapping = ggplot2::aes(x = sog), binwidth = 0.2)
# 
# sog_explore

```

Spike at \< 5knots, and some additional noise over 25 knots. I want to know more about the distribution in these areas, what vessels are travelling at such low speeds and plot out where these low speeds tend to be (I have a feeling it will be in port), and then what vessels are travelling at high speeds (I think these could be enforcement or anomaly but lets test).

```{r}
# high_speed = matched_wras_ais %>% 
#   ggplot2::ggplot() +
#   ggplot2::geom_histogram(mapping = ggplot2::aes(x = sog), binwidth = 0.1) +
#   ggplot2::coord_cartesian(xlim = c(26, 110))
# 
# high_speed
# 
# high_speed_data = matched_wras_ais %>% 
#   dplyr::filter(sog > 25) %>% 
#   dplyr::select(c(name, mmsi, type, sog, status))
```

The high speed craft generally travel at \<\~40 knots, although there is one case of the speed tipping into 40.3 knots. Anything above this is errors and states that vessel traveled in excess of 100 knots. This can be removed.

```{r}
# low_speed = matched_wras_ais %>%
#   ggplot2::ggplot(.) + 
#   ggplot2::geom_histogram(mapping = ggplot2::aes(x = sog), binwidth = 0.1) +
#   ggplot2::coord_cartesian(xlim = c(0, 10))
# 
# low_speed
# 
# low_speed_data = matched_wras_ais %>%
#   dplyr::group_by(alert_sent) %>% 
#   dplyr::filter(any(sog <= 0.2)) %>% ## This tests if this value appears in a group (any) and filters it out. Example below of how it works.
#   dplyr::select(local_time, alert_sent, name, mmsi, skipper, type, sog, status, latitude.x, longitude.x) %>% 
#   dplyr::arrange(alert_sent)

```

Large cluster approaching 0. Why? I imagine that the majority of these will be within port boundaries, or near mooring buoys/tug areas. I can make a map to test this.

```{r}

# low_speed_map = matched_wras_ais %>% 
#   dplyr::filter(sog < 0.2)  
# 
# leaflet::leaflet(data = low_speed_map, options = leaflet::leafletOptions(preferCanvas = TRUE)) %>% 
#   leaflet::addProviderTiles(leaflet::providers$OpenStreetMap, options = leaflet::providerTileOptions(
#   updateWhenZooming = FALSE,      # map won't update tiles until zoom is done
#   updateWhenIdle = TRUE           # map won't load new tiles when panning
#   )) %>% 
#   leaflet::addMarkers(lng = ~longitude.x, lat = ~latitude.x,
#                             popup = ~(paste(name, type)),
#                             clusterOptions = leaflet::markerClusterOptions())
# 
# leaflet::leaflet(data = low_speed_map, options = leaflet::leafletOptions(preferCanvas = TRUE)) %>% 
#   leaflet::addTiles(options = leaflet::tileOptions(updateWhenZooming = FALSE,
#                                                          updateWhenIdle = TRUE)) %>% 
#   leaflet::addMarkers(lng = ~longitude.x, lat = ~latitude.x)


```

Vessels travelling already at around 7-10knots will (or even probably slightly faster) will not slow down for whales unless absolutely necessary. Personal experience and anecdotal evidence suggests that it does happen, particularly closer to port, but it isn't likely. I suggest filtering data \<= 10knots and \> 40 knots.

Filter out all of the data under 0.2 knots + over 40.2 knots. Group by class and calculate difference to median for speed of vessels.

```{r}

# clean_wras_ais_filtered = matched_wras_ais %>%
#   dplyr::filter(sog > 10 & sog < 40) %>%
#   dplyr::mutate(type = dplyr::case_when(stringr::str_detect(type, "Tanker") ~ "Tanker",
#                                         stringr::str_detect(type, "Cargo ship") ~ "Cargo ship",
#                                         stringr::str_detect(type, "Other") ~ "Other",
#                                         TRUE ~ type))

# clean_wras_ais_non_filtered = matched_wras_ais %>%
#   dplyr::filter(sog < 40.2) %>% 
#   dplyr::mutate(type = dplyr::case_when(stringr::str_detect(type, "Tanker") ~ "Tanker",
#                                         stringr::str_detect(type, "Cargo ship") ~ "Cargo ship",
#                                         stringr::str_detect(type, "Other") ~ "Other",
#                                         TRUE ~ type))

```

Is there some groups that just don't need to be included as they are so small?

data_count = clean_wras_ais_filtered %\>%

dplyr::group_by(type) %\>%

dplyr::summarise(count = dplyr::n_distinct(alert_sent))

```{r}
# data_count = clean_wras_ais_filtered %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(count = dplyr::n_distinct(alert_sent))

```

Filter out everything which is under 100 sample size, leaving us with..

```{r}
# vessel_type_filter = data_count %>% dplyr::filter(count > 100) %>% .$type
# vessel_type_filter
```

Left with "Passenger Ship", "HSC" (high speed craft), and "Cargo ship", totaling 3394 alerts.

#### Takeaways

-   We need to filter the data by vessels \> 7 knots and \< 40 knots

-   We will filter the groups of vessel types which have over 100 records, this includes HSC, Passenger ships, and Cargo Ships

## Streamlining baseline data area

-   I want to see where vessels are travelling when they recieved alerts, do patters (dependant of vessel type) emerge, can I use those to fine tune the study?

```{r}
# baseline_data %>%
#   dplyr::group_by(date, mmsi) %>% 
#   dplyr::summarise(latitude = mean(latitude), longitude = mean(longitude)) %>% 
#   sf::st_as_sf(coords = c("longitude", "latitude")) %>%
#   leaflet::leaflet() %>% 
#   leaflet::addTiles() %>% 
#   leaflet::addCircleMarkers(radius = 0.5) %>% 
#   leaflet::addCircleMarkers(data = for_analysis, 
#                              lat = for_analysis$latitude.x,
#                              lng = for_analysis$longitude.x,
#                              radius = 0.5, col = "red")


```

-   The above shows that the baseline data has a much wider extent than the analysis data. I need to take area out as a variable in the speed study. In order to do this I will set an area for the study with a custom polygon and look within this range instead.
    -   For now I am just using the Salish Sea (ish) as a polygon, but I will expand this to be Seattle...

```{r}
# data_filter_shp = sf::st_read("../../../../Ocean Wise Conservation Association/Project - WRAS - Efficacy/Resources/efficacy-area-filter.shp") %>% 
#   sf::st_set_crs(4326)

data_filter_shp = sf::st_read("../../../../Ocean Wise Conservation Association/Project - WRAS - Efficacy/Resources/efficacy-area-filter-extended.shp") %>% 
  sf::st_set_crs(4326) 


# baseline_data %>%
#   dplyr::group_by(date, mmsi) %>% 
#   dplyr::summarise(latitude = mean(latitude), longitude = mean(longitude)) %>% 
#   sf::st_as_sf(coords = c("longitude", "latitude")) %>%
#   leaflet::leaflet() %>% 
#   leaflet::addTiles() %>% 
#   leaflet::addCircleMarkers(radius = 0.5) %>% 
#   leaflet::addPolygons(data = data_filter_shp)


## Define chunk size 
chunk_size <- 10000

# Split your data into chunks
num_chunks = ceiling(nrow(baseline_data) / chunk_size)
chunks <- split(baseline_data, rep(1:num_chunks, each = chunk_size, length.out = nrow(baseline_data)))

## Define a function for spatial join
perform_spatial_join <- function(chunk) {
  
  chunk_sf <- sf::st_as_sf(chunk, coords = c("longitude", "latitude"), crs = sf::st_crs(data_filter_shp))
  
  ## Spatial join using chunk's spatial index
  # joined_data <- sf::st_join(chunk_sf, data_filter_shp, join = sf::st_within) # left = FALSE, right = FALSE, use_sindex = TRUE)
  joined_data = chunk_sf%>% 
    dplyr::mutate(intersection = as.integer(sf::st_intersects(geometry, data_filter_shp))) %>% 
    dplyr::filter(is.na(intersection)==F)
  
  return(joined_data)
}

filtered_baseline_data = purrr::map(chunks, perform_spatial_join) %>%
  dplyr::bind_rows()

filtered_analysis_data = sf::st_as_sf(for_analysis, 
                                      coords = c("longitude.y", "latitude.y"), 
                                      crs = sf::st_crs(data_filter_shp)) %>% 
  dplyr::mutate(intersection = as.integer(sf::st_intersects(geometry, data_filter_shp))) %>% 
  dplyr::filter(is.na(intersection)==F)

filtered_baseline_data %>%
  leaflet::leaflet() %>%
  leaflet::addTiles() %>%
  leaflet::addCircleMarkers(radius = 0.5) %>%
  leaflet::addCircleMarkers(data = filtered_analysis_data, radius = 0.5, col = "red")


 
```

-   Now i've matched the WRAS data and the baseline data, I can repeat my speed analysis and see if there is a difference.
    -   WRAS data is around 50% reduced, baseline data is around 83% reduced

```{r}

median_filtered_wras_data = filtered_analysis_data %>% 
  dplyr::group_by(type) %>% 
  dplyr::summarise(median_test = median(sog)) %>% 
  sf::st_drop_geometry()

median_filtered_baseline_data = filtered_baseline_data %>% 
  dplyr::filter(type %in% median_filtered_wras_data$type) %>% 
  dplyr::group_by(type) %>% 
  dplyr::summarise(median_baseline = median(sog)) %>%
  sf::st_drop_geometry()

baseline_vs_wras = filtered_analysis_data %>% 
  dplyr::filter(type %in% median_filtered_wras_data$type) %>% 
  dplyr::left_join(median_filtered_baseline_data, by = "type") %>% 
  dplyr::left_join(median_filtered_wras_data, by = "type") %>% 
  dplyr::group_by(type) %>%
  ggplot2::ggplot(ggplot2::aes(x = sog)) + 
  ggplot2::geom_histogram(binwidth = 1) + 
  # ggplot2::geom_histogram(data = filtered_analysis_data, binwidth = 1, color = "purple") +
  ggplot2::facet_wrap(~type, scales = "free") +
  ggplot2::geom_vline(ggplot2::aes(xintercept = median_baseline), color = "red", linetype = "dashed") + 
  ggplot2::geom_vline(ggplot2::aes(xintercept = median_test), color = "darkgrey", linetype = "solid") + 
  ggplot2::labs(x = "SOG", y = "Frequency") + 
  ggplot2::theme_minimal()

baseline_vs_wras


```

## Types of vessels...

-   I've noticed that in the AIS data there are many sub-categories of vessels found within "Passenger ship", predominantly small eco-tourism vessels. These vessels aren't covered under WRAS access, so in order to match the baseline data to vessels within WRAS I will have to come up with a way of filtering out these vessels from the baseline data.

-   My first thought is to look at length to see if there is something I can do there...

    -   I will make plots of the distribution of length from baseline data and WRAS data and see what comes out.

```{r}
library(patchwork)  

baseline_plot = filtered_baseline_data %>% 
  dplyr::filter(type %in% unique(filtered_analysis_data$type)) %>% 
  ggplot2::ggplot(data = ., ggplot2::aes(x = type, y = length, fill = type)) +   
  ggplot2::geom_boxplot(show.legend = F) +   
  ggplot2::theme_minimal() +
  ggplot2::coord_cartesian(ylim = c(0, 520)) +
  ggplot2::ggtitle("Baseline")

analysis_plot = filtered_analysis_data %>%    
  ggplot2::ggplot(data = ., ggplot2::aes(x = type, y = length, fill = type)) +   
  ggplot2::geom_boxplot(show.legend = F) +   
  ggplot2::theme_minimal() +
  ggplot2::coord_cartesian(ylim = c(0, 520)) + 
  ggplot2::ggtitle("WRAS")

combined_plots = baseline_plot + analysis_plot

combined_plots
```

## 

-   Looking at the above plot we can see that the distribution of lengths for cargo and passenger ships is different between the baseline and the analysis data. This suggests a difference in vessel types within these broad categories which could mean a difference in speeds. To check this I'll look at a scatter plot.

    -   I'm more focused on the passenger vessels rather than the cargo ships. If I remove outliers and match the min vessel length for the baseline to the min length of the WRAS data we will have a pretty good standardization for this group.

    -   Passenger vessels is a bit trickier in my opinion so I will try to better understand the data before filtering.

```{r}

filtered_baseline_data %>% 
  dplyr::filter(type %in% unique(filtered_analysis_data$type)) %>% 
  dplyr::group_by(type) %>% 
  ggpubr::ggscatter(., x = "sog", y = "length", col = "type")
  

  
```

-   Looks like my prediction was correct. In Passenger vessels, smaller vessels travel faster than larger vessels.

    -   I should filter this passenger data down to match the analysis data.

-   I'm going to run a quick plot of the WRAS dataset to see the differences...

```{r}
filtered_analysis_data %>% 
  dplyr::filter(type %in% unique(filtered_analysis_data$type)) %>% 
  dplyr::group_by(type) %>% 
  ggpubr::ggscatter(., x = "sog", y = "length", col = "type")
```

-   We can see by the above scatter plots that there is a distinct weighting of baseline data towards smaller, faster vessels for passenger type vessels (and to a lesser extent but still present in cargo vessels).

-   I am removing outliers via the **interquartile range method**

-   To dos...

    -   [x] Compare distribution of speed vs length

    -   [x] Clean the cargo ship data to remove outliers and smaller vessels than the min of the test group.

    -   [x] Clean passenger category - smaller vessels to be removed (based on scatter plot)

    -   [x] re run analysis

### Removing outliers and filtering baseline data

```{r}

## Remove outliers 
clean_analysis_data = filtered_analysis_data %>% 
  dplyr::filter(length < 230 & length > 70 & type == "Passenger ship" | type == "Cargo ship")

## Set the limits of the baseline data to the analysis data 
clean_baseline_data = filtered_baseline_data %>% 
  dplyr::filter(length <= max(clean_analysis_data$length) &
                  length >= min(clean_analysis_data$length),
                type %in% clean_analysis_data$type)
  
median_filtered_wras_data = clean_analysis_data %>% 
  dplyr::group_by(type) %>% 
  dplyr::summarise(median_test = median(sog)) %>% 
  sf::st_drop_geometry()

median_filtered_baseline_data = clean_baseline_data %>% 
  dplyr::filter(type %in% median_filtered_wras_data$type) %>% 
  dplyr::group_by(type) %>% 
  dplyr::summarise(median_baseline = median(sog)) %>%
  sf::st_drop_geometry()

baseline_vs_wras = clean_analysis_data %>% 
  dplyr::filter(type %in% median_filtered_wras_data$type) %>% 
  dplyr::left_join(median_filtered_baseline_data, by = "type") %>% 
  dplyr::left_join(median_filtered_wras_data, by = "type") %>% 
  dplyr::group_by(type) %>%
  ggplot2::ggplot(ggplot2::aes(x = sog)) + 
  ggplot2::geom_histogram(binwidth = 1) + 
  ggplot2::facet_wrap(~type, scales = "free") +
  ggplot2::geom_vline(ggplot2::aes(xintercept = median_baseline), color = "darkgrey", linetype = "solid") + 
  ggplot2::geom_vline(ggplot2::aes(xintercept = median_test), color = "red", linetype = "dashed") + 
  ggplot2::labs(x = "SOG", y = "Frequency") + 
  ggplot2::theme_minimal()

```

-   Not that I am hunting for answers, but I would like to see whether there is another factor impacting the vessel speed for the passenger vessels.

    -   is the WRAS data including High Speed Ferries in here when the Baseline is not?

    -   Is there a major difference in routes that are being compared with the WRAS and Baseline?

# Sandbox

### Baseline Data

```{r eval=FALSE}
  # dplyr::filter(sog < 40.2)
  # dplyr::filter(sog > 0.2 & sog < 40.2)

# median_baseline_filtered = baseline_data %>% 
#   # dplyr::filter(sog > 7) %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(median = median(sog))
# 
# median_baseline_non_filtered = baseline_data %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(median = median(sog))
# 
# 
# baseline_plots_no_lower = baseline_data %>% 
#   dplyr::filter(sog > 7) %>% 
#   dplyr::left_join(median_baseline_filtered, by = "type") %>% 
#   dplyr::group_by(type) %>%  
#   ggplot2::ggplot(ggplot2::aes(x = sog)) +
#   ggplot2::geom_histogram(binwidth = 1) +
#   ggplot2::geom_vline(ggplot2::aes(xintercept = median), color = "red", linetype = "dashed") +
#   ggplot2::facet_wrap(~type, scales = "free") +
#   ggplot2::labs(x = "SOG", y = "Frequency") +
#   ggplot2::theme_minimal()
# 
# baseline_plots_lower = baseline_data %>% 
#   dplyr::left_join(median_baseline_non_filtered, by = "type") %>% 
#   dplyr::group_by(type) %>%  
#   ggplot2::ggplot(ggplot2::aes(x = sog)) +
#   ggplot2::geom_histogram(binwidth = 1) +
#   ggplot2::geom_vline(ggplot2::aes(xintercept = median), color = "red", linetype = "dashed") +
#   ggplot2::facet_wrap(~type, scales = "free") +
#   ggplot2::labs(x = "SOG", y = "Frequency") +
#   ggplot2::theme_minimal()
# 
# ggpubr::ggarrange(baseline_plots_no_lower, baseline_plots_lower,
#                   labels = c("filtered", "non-filtered"),
#                   nrow = 2)


clean_baseline_data %>%
  leaflet::leaflet() %>%
  leaflet::addTiles() %>%
  leaflet::addCircleMarkers(radius = 0.5) %>%
  leaflet::addCircleMarkers(data = clean_analysis_data, radius = 0.5, col = "red")



for_analysis %>% 
  leaflet::leaflet() %>% 
  leaflet::addTiles() %>% 
  leaflet::addCircleMarkers(data = for_analysis, lat = ~latitude.x, lng = ~longitude.x, radius = 0.5, col = "purple") %>% 
  leaflet::addPolygons(data = data_filter_shp$geometry)

```
