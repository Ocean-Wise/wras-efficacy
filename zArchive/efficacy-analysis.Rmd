---
title: "WRAS Efficacy Analysis"
author: "Alex Mitchell"
date: "2023-05-15"
output: 
  rmdformats::downcute:
    downcute_theme: "chaos"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### No QA has been done.

# Introduction

The Whale Report Alert System, running since 2018, has not been empirically tested to assess the effectiveness of the tool. In order to assess our own tool, push for further funding, and show the effectiveness to international partners we need to have evidence of its success.

Therefore the aims of the project are to...

-   Use survey data to understand more about mariners behavior when using the WRAS from their perspective

-   Use AIS and WRAS alert data to test how often a vessel changes speed or direction due to a WRAS alert

## Data

### Ocean Wise

-   WRAS Alert data
    -   This comes from an extract of the database from Lapis. It contains information on all users who received an alert from a sighting.
    -   This does not include operations center staff/desktop application users.
-   WRAS User data
    -   Some users should not be included in the analysis, for example Ocean Wise staff. We can use this to filter out users we want to ignore.
-   Vessel data
    -   Contains information on all the vessels logged through the WRAS alerts data.
    -   Very messy, containing a lot of user error in entering data.
    -   I have spent hours on MarineTraffic identifying vessels and additional information.
    -   We need to join the vessel data with the WRAS data as we will use the MMSI number of the vessel to enable us to use WRAS alert data with the AIS data.

### Vessel position data

-   AIS
    -   Extract from Ocean Networks Canada (ONC - Kiyomi Holman).
    -   Already pre-processed by ONC.
    -   Columns: UTC Time \| Local \| Time \| MMSI \| IMO \| Name \| Type \| Length \| Beam \| Draught \| SOG \| COG \| Heading \| Latitude \| Longitude \| Destination \| Status \| Flag
        -   We will use the SOG (speed over ground) and status to determine speed changes, and lat/lng to determine change in direction.

### Survey data

-   Microsoft form sent out by Ocean Wise to WRAS users.
    -   TBD

# Data Cleaning

```{r warning = F, message = F}
library(magrittr)
library(lubridate) ## We need to load this specifically for the %within% function

## Edit this if you're running on your own Ocean Wise machine. 
user = "AlexMitchell"
```

In this section, we will perform data cleaning tasks to prepare the dataset for analysis. Describe the steps taken to clean the data, such as handling missing values, removing duplicates, correcting inconsistent values, or transforming variables.

## Load Data

### WRAS Alert Data

```{r  warning = F, message = F}
alerts = readxl::read_xlsx(paste0("C:/Users/",
                                             user,
                                             "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/WRAS/WRAS_main.xlsx")) %>% 
  janitor::clean_names()
```

### WRAS Users

```{r warning = F, message = F}
users = readxl::read_xlsx(paste0("C:/Users/",
                                       user,
                                       "/Ocean Wise Conservation Association/Whales Initiative - General/BCCSN.Groups/WhaleReport Alert System/Participants/WRASUSERS_main.xlsx"), sheet = "Authorized") %>% 
  janitor::clean_names()
```

### Vessel data

```{r warning = F, message = F}
vessels = readxl::read_xlsx(paste0("C:/Users/",
                                             user,
                                             "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/WRAS/Vessels_main.xlsx")) %>% 
  janitor::clean_names()

```

## Data Cleaning Tasks

Perform specific data cleaning tasks, such as handling missing values, removing duplicates, correcting inconsistent values, or transforming variables. Provide code examples and explanations for each task.

### Users

-   Relatively simple cleaning by removing special characters from all columns barring emails.

```{r warning = F, message = F}
users_clean = users %>% 
  dplyr::mutate(dplyr::across(!email, fedmatch::clean_strings)) %>%  ## cleans strings to remove special characters and formatting (all cols but email)
  dplyr::mutate(name = paste(first_name, last_name)) %>% 
  dplyr::select(name, organization, org_type, email, region_clean, include_in_efficacy_analysis)
  
```

### Vessels

-   Splitting the `loa` and `be` column to create a length and breadth columns and extracting the numbers from those columns to remove odd characters.
-   I added a `Key2` column in here which is just a row number, this is needed for the `fuzzy_matching` later on.
-   Removing the "mv" and "m v" parts of strings as they appear

```{r warning = F, message = F}
vessels_clean = vessels %>%
  dplyr::mutate(dplyr::across(!loa_x_be, fedmatch::clean_strings)) %>%
  tidyr::separate(loa_x_be, c("loa", "be"),sep = "x") %>%  ## split loa x be column to two seperate columns
  dplyr::mutate(dplyr::across(c(loa, be), readr::parse_number)) %>%  ## extract numerics from the new columns
  dplyr::mutate(key2 = 1:nrow(.)) %>% 
  dplyr::mutate(vessel_name = stringr::str_remove_all(vessel_name, "mv |m v ")) %>% 
  dplyr::select(vessel_name, mmsi, imo, class, loa, be, notes, key2)
```

### Alerts

-   Clean strings to remove special characters
-   Filter out the Ocean Wise vessels
-   Again, create a key for the fuzzy matching that happens shortly
-   Remove acronyms before vessel names due to them only partially being used
-   We need to adjust some of the names of vessels as the fuzzy matching can't handle the miss-spellings. This is a manual process and will need to be updated when we get more WRAS alerts.

```{r warning = F, message = F}
alerts_clean = alerts %>% 
  dplyr::select(c("alert_sent",
                  # "sighted_at",
                  "latitude", 
                  "longitude", 
                  "skipper", 
                  "vessel", 
                  "species")) %>% 
  dplyr::mutate(dplyr::across(!longitude & !latitude & !alert_sent, fedmatch::clean_strings), ## cleans strings to remove special characters and formatting
                alert_sent = lubridate::force_tz(alert_sent, "Canada/Pacific")) %>%  ## I had to force the time zone for the later interval to be in the correct time zone as well. 
  dplyr::filter(!vessel %in% c("skana", "kellahan","tsitika","no vessel", 
                               "land",
                               "on land",
                               "pilot underway")) %>%
  ## Assigns a unique key to the data set for the fuzzy matching later
  dplyr::mutate(key1 = 1:nrow(.)) %>% 
  dplyr::mutate(vessel = stringr::str_remove_all(vessel, "mv |m v ")) %>% 
  ## The next mutates clean up some of the common spelling errors and bits the fuzzy matching misses. Repeated for vessel data.
  dplyr::mutate(vessel = dplyr::case_when(vessel == "charles hays amwaal" ~ "charles hays",
                                          vessel == "amwaal" ~ "charles hays",
                                          vessel == "cowichan" ~ "queen of cowichan",
                                          vessel == "sobc" ~ "spirit of british columbia",
                                          vessel == "qalb" ~ "queen of alberni",
                                          vessel == "spirit of bc" ~ "spirit of british columbia",
                                          vessel == "reliant" ~ "seaspan reliant",
                                          vessel == "queen of newwest" ~ "queen of new westminster",
                                          vessel == "q of alberni" ~ "queen of alberni",
                                          vessel == "mazuru bishamon" ~ "maizuru bishamon",
                                          vessel == "coastal renn" ~ "coastal renaissance",
                                          vessel == "sovc" ~ "spirit of vancouver island",
                                          vessel == "q of alberni" ~ "queen of alberni",
                                          vessel == "qnw" ~ "queen of new westminster",
                                          vessel == "laurier" ~ "sir wilfrid laurier",
                                          vessel == "suquamish" ~ "wsf suquamish",
                                          vessel == "howe sounbd queen" ~ "howe sound queen",
                                          vessel == "inspiration" ~ "coastal inspiration",
                                          vessel == "suquwamish" ~ "wsf suquamish",
                                          vessel == "seapan zambizi" ~ "seaspan zambezi",
                                          vessel == "sprit of british columbia" ~ "spirit of british columbia",
                                          vessel == "roald almundsen" ~ "roald amundsen",
                                          vessel == "ovean clio" ~ "ocean clio",
                                          vessel == "coroleader ol" ~ "coreleader ol",
                                          vessel == "zuidetdam" ~ "zuiderdam",
                                          vessel == "seaspan anadonis" ~ "seaspan adonis",
                                          vessel == "berge yotie" ~ "berge yotei",
                                          vessel == "carnval splendor" ~ "carnival splendor",
                                          vessel == "blackball" ~ "coho",
                                          vessel == "zeta" ~ "star zeta",
                                          vessel == "cma cgm rigalito" ~ "cma cgm rigoletto",
                                          vessel == "gulf islands spirit" ~ "spirit of vancouver island",
                                          vessel == "zeda" ~ "star zeta",
                                          vessel == "zeta" ~ "star zeta",
                                          TRUE ~ vessel)) %>% 
  dplyr::mutate(dplyr::across(!c(latitude, longitude, alert_sent),fedmatch::clean_strings)) 


```

# Data Processing

In this section, we will process the cleaned data to extract relevant information, create new variables, or derive insights. Describe the steps involved in data processing and any transformations or calculations performed.

## Data Transformation

### Matching WRAS and Vessel data

WRAS alert data does not contain MMSI number of vessel which we need to use to identify vessel tracks in AIS. WRAS alert data contains spelling mistakes in the vessel name due to human error. We need a way to match the WRAS alert data to the vessel data, and we can only do that by name. Fuzzy matching will allow us to account for human error in spelling in WRAS data to match vessel names in our main vessel dataset. These settings have been changed a little bit, with the maxDist set at 0.5 we were getting incorrect matches The "checkMatch" object contains a df of vessels names which don't exactly match. I manually checked through this. Dataframe and manually renamed vessel names in the WRAS_data_import. With this many vessel names some are too close (in string distance) for the function to accurately match correctly in all instances.

```{r warning = F, message = F}
WRAS_data_matched <- fedmatch::merge_plus(data1 = alerts_clean, 
                                          match_type = "fuzzy",
                                          data2 = vessels_clean,
                                          by.x = "vessel", by.y = "vessel_name",
                                          unique_key_1 = "key1", unique_key_2 = "key2", 
                                          suffixes = c("_1", "_2"),
                                          fuzzy_settings = fedmatch::build_fuzzy_settings(maxDist = 0.1))


## List of vessels without MMSI numbers - uncomment to view the dataset
# View(tibble::as_tibble(unique(WRAS_data_matched$data1_nomatch$vessel)))

## Cleaned data
WRAS_data_midclean <- WRAS_data_matched$matches


```

The final steps before we have cleaned alert data ready to process the AIS data -

-   Filter for users from Ocean Wise and other non-mariner organizations who should not be included in the efficacy analysis - they would likely not take action to a WRAS alert.

-   Create a time interval of before and after an alert was sent to compare during analysis. This will also be used to reduce the size of the AIS data via a join later on.

    -   Imagining a scenario where a vessel is on the edge of the alert radius, the epicenter being 5nm away (total diameter of an alert is 10nm). In 20mins a vessel, travelling at 13.5knots, will reach the alert centre.

    -   For this reason, I have chose 20min as a good guess for how long it would take for a vessel to slow down and stay slowed, any longer we might see and uptick in speed.

-   The `include_in_efficacy_analysis` filter removes Ocean Wise staff from the analysis.

```{r warning = F, message = F}
user_filter = users_clean$name[users_clean$include_in_efficacy_analysis == "no"]

WRAS_data_cleaned = WRAS_data_midclean %>% 
  dplyr::filter(!skipper %in% user_filter) %>% 
  dplyr::select(-c(key1, key2, notes, tier)) %>% 
  dplyr::mutate(mmsi = as.numeric(mmsi)) %>% 
  # create an interval 30mins before and after an alert was sent.
  dplyr::mutate(range = lubridate::interval(alert_sent - lubridate::minutes(30),
                                            alert_sent + lubridate::minutes(30)),
                date = lubridate::as_date(alert_sent))

    
                 

## Remove uneccesary objects from environment to reduce storage in memory.
rm(WRAS_data_matched, WRAS_data_midclean)


```

## AIS data

Given how large this data is (\~ 6-7GB per folder), and that it is split into weekly files, we will have to write a loop to go over the data, clean it and process it at the same time. The data will be loaded and processed later on after creating a cleaned alerts dataset.

The AIS data is already in a clean format, but we need to process it to remove ships which are a particular status (this [link](https://datalastic.com/blog/ais-navigational-status/){style="font-size: 11pt;"} describes the categories of AIS *status*), as well as create a interval before and after alerts to reduce the data size to only relevant data. Processing steps:

-   Filter to remove ships which are:

    -   At anchor;

    -   Moored;

    -   Not under control;

    -   Restricted maneuverability;

    -   Pushing or towing alongside;

    -   Engaged in fishing;

-   Join data if the MSSI matches, the date matches, a the time is -30 or +30 mins from the time of the alert.

Aim of this cleaning is to reduce the data size drastically to only relevant AIS tracks and vessels which are moving without assistance or hindrance.

```{r warning = F, message = F}

status_filter = list("Moored", "Not under command", "Engaged in fishing", "At anchor", "Pushing or towing alongside", "Restricted manoeuvrability")

## Create a list of folder paths to loop a function over to get all individual file names.
folder_paths = c(paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2019"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2020"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2021"),
                paste0("C:/Users/",
                            user,
                            "/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2022"))


## Function to get all file names in folders
get_file_names = function(folder_path){
  files = list.files(path = folder_path, full.names = T)
}

## Create a list of all file names I want to iterate a function over
file_list = purrr::map(folder_paths, get_file_names) %>% 
  unlist()
  

## Loop function to load AIS data, clean it, mutate columns to correct format then join to the WRAS alerts data.

loop_function = function(file_path) {
  ## Going to use data.table::fread to load as it is exponentially faster than read.csv.
  ais_data = data.table::fread(file_path) %>% 
    janitor::clean_names() %>% 
    dplyr::filter(!status %in% status_filter) %>%
    dplyr::mutate(local_time = lubridate::force_tz(lubridate::as_datetime(local_time), tzone = "Canada/Pacific"),
                  date = lubridate::date(local_time))
  

  matched_alerts = ais_data %>% 
    dplyr::inner_join(WRAS_data_cleaned, by = c("date", "mmsi")) %>% 
    dplyr::mutate(match = dplyr::case_when(local_time %within% range ~ "Y",
                                           TRUE ~ "N")) %>% 
    dplyr::filter(match == "Y") ## If the AIS data falls within the alert interval we set earlier (and labelled it above with "Y" then keep these rows).

}

```

Now that we have our have created a list of files from within the folders of AIS data between 2019 - 2022 we can loop over these file names and join to our WRAS data creating our final dataset.

```{r warning = F, message = F}
matched_wras_ais = purrr::map(file_list, loop_function) %>%
  purrr::discard(~ nrow(.x) == 0) %>% 
  purrr::reduce(dplyr::full_join, 
                by = dplyr::join_by(utc_time, 
                                    local_time, 
                                    mmsi, 
                                    imo.x, 
                                    name, 
                                    type, 
                                    length, 
                                    beam, 
                                    draught, 
                                    sog, 
                                    cog, 
                                    heading, 
                                    latitude.x, 
                                    longitude.x, 
                                    destination,
                                    status, 
                                    flag, 
                                    date, 
                                    alert_sent, 
                                    latitude.y, 
                                    longitude.y, 
                                    skipper, 
                                    species, 
                                    vessel, 
                                    vessel_name, 
                                    imo.y, 
                                    class, 
                                    loa, 
                                    be, 
                                    range, 
                                    match)) 

```

## Baseline data

Need to get some baseline data to compare our post-WRAS alert vessel behavior to the average behavior of a vessel. To do this I am going to use the 2019 data given that shipping would've been at pre-pandemic levels, and WRAS engagement would've been relatively (to now) low being a new tool.

```{r}
## function to loop through 2019 data and do some cleaning

# baseline_files = list.files(paste0("C:/Users/", user,"/Ocean Wise Conservation Association/Whales Initiative - General/Ocean Wise Data/AIS/2019"), full.names = T)

baseline_loop_function = function(file_path) {
  baseline_data = data.table::fread(file_path) %>% 
    janitor::clean_names() %>% 
    dplyr::filter(!status %in% status_filter) %>%
    dplyr::mutate(local_time = lubridate::force_tz(lubridate::as_datetime(local_time), tzone = "Canada/Pacific"),
                  date = lubridate::date(local_time)) %>% 
    dplyr::select(c("date", "mmsi", "name", "type", "length", "beam", "sog", "cog", "heading", "latitude", "longitude"))

  ## This removes vessels that have recieved alerts...
  remove_alerts = baseline_data %>% 
    dplyr::anti_join(WRAS_data_cleaned, by = c("date", "mmsi")) %>%
    dplyr::mutate(type = dplyr::case_when(stringr::str_detect(type, "Tanker") ~ "Tanker",
                                          stringr::str_detect(type, "Cargo ship") ~ "Cargo ship",
                                          stringr::str_detect(type, "Other") ~ "Other",
                                          TRUE ~ type)) %>% 
    dplyr::filter(type %in% c("Cargo ship", "HSC", "Passenger ship"))
}

```

We will now loop over the files in the 2019 folder, filter out any vessels which received WRAS alerts in this time.

2024-07-08:

-   I want to

```{r}
baseline_file_list = sample(file_list, 52)

baseline_data = purrr::map(baseline_file_list, baseline_loop_function) %>%
  purrr::reduce(dplyr::full_join, by = c("date", "mmsi", "name", "type", "length", "beam", "sog", "cog", "heading", "latitude", "longitude")) %>% 
  dplyr::filter(lubridate::year(date) != 2018) %>% 
  dplyr::filter(sog > 7 & sog < 40)




```

## Final Processing from leanings in the "Explore" section

1.  Filter data speed to reflect realistic expectations of vessel traffic speed, as well as to speeds at which vessels would potentially take action on an alert (7 - 40 knots).
2.  Remove any category of vessel with \< 100 record to create a large sample size.

```{r}

clean_wras_ais_filtered = matched_wras_ais %>%
  dplyr::filter(sog > 7 & sog < 40) %>%
  dplyr::filter(heading >= 0 & heading <= 360) %>%
  dplyr::mutate(type = dplyr::case_when(stringr::str_detect(type, "Tanker") ~ "Tanker",
                                        stringr::str_detect(type, "Cargo ship") ~ "Cargo ship",
                                        stringr::str_detect(type, "Other") ~ "Other",
                                        TRUE ~ type))

data_count = clean_wras_ais_filtered %>% 
  dplyr::group_by(type) %>% 
  dplyr::summarise(count = dplyr::n_distinct(alert_sent))

## "search and rescue" will be removed as they may produce erronous results given behaviour (emergency situation).

vessel_type_filter = data_count %>% dplyr::filter(count >= 100) %>% .$type

for_analysis = clean_wras_ais_filtered %>% 
  dplyr::filter(type %in% vessel_type_filter)




```

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

# Exploration

We need to further explore the data to see whether we can eek out any other necessary cleaning of outlines, or identify some questions to answer.

For the efficacy analysis we are interested in testing whether or not we see a change in speed of direction which could be attributed to an event (WRAS alert). Speed is relatively straight forward, direction is not.

## Speed over ground

I want to see how SOG is distributed through out our data, and if there are any values we can trim out, for instance extremely low speeds (\<3knots maybe) which would indicate a vessel arriving at port, or any high speeds which may be an AIS malfunction.

```{r}

# sog_explore = matched_wras_ais %>% 
#   ggplot2::ggplot(data = .) +
#   ggplot2::geom_histogram(mapping = ggplot2::aes(x = sog), binwidth = 0.2)
# 
# sog_explore

```

Spike at \< 5knots, and some additional noise over 25 knots. I want to know more about the distribution in these areas, what vessels are travelling at such low speeds and plot out where these low speeds tend to be (I have a feeling it will be in port), and then what vessels are travelling at high speeds (I think these could be enforcement or anomaly but lets test).

```{r}
# high_speed = matched_wras_ais %>% 
#   ggplot2::ggplot() +
#   ggplot2::geom_histogram(mapping = ggplot2::aes(x = sog), binwidth = 0.1) +
#   ggplot2::coord_cartesian(xlim = c(26, 110))
# 
# high_speed
# 
# high_speed_data = matched_wras_ais %>% 
#   dplyr::filter(sog > 25) %>% 
#   dplyr::select(c(name, mmsi, type, sog, status))
```

The high speed craft generally travel at \<\~40 knots, although there is one case of the speed tipping into 40.3 knots. Anything above this is errors and states that vessel traveled in excess of 100 knots. This can be removed.

```{r}
# low_speed = matched_wras_ais %>%
#   ggplot2::ggplot(.) + 
#   ggplot2::geom_histogram(mapping = ggplot2::aes(x = sog), binwidth = 0.1) +
#   ggplot2::coord_cartesian(xlim = c(0, 10))
# 
# low_speed
# 
# low_speed_data = matched_wras_ais %>%
#   dplyr::group_by(alert_sent) %>% 
#   dplyr::filter(any(sog <= 0.2)) %>% ## This tests if this value appears in a group (any) and filters it out. Example below of how it works.
#   dplyr::select(local_time, alert_sent, name, mmsi, skipper, type, sog, status, latitude.x, longitude.x) %>% 
#   dplyr::arrange(alert_sent)

```

Large cluster approaching 0. Why? I imagine that the majority of these will be within port boundaries, or near mooring buoys/tug areas. I can make a map to test this.

```{r}

# low_speed_map = matched_wras_ais %>% 
#   dplyr::filter(sog < 0.2)  
# 
# leaflet::leaflet(data = low_speed_map, options = leaflet::leafletOptions(preferCanvas = TRUE)) %>% 
#   leaflet::addProviderTiles(leaflet::providers$OpenStreetMap, options = leaflet::providerTileOptions(
#   updateWhenZooming = FALSE,      # map won't update tiles until zoom is done
#   updateWhenIdle = TRUE           # map won't load new tiles when panning
#   )) %>% 
#   leaflet::addMarkers(lng = ~longitude.x, lat = ~latitude.x,
#                             popup = ~(paste(name, type)),
#                             clusterOptions = leaflet::markerClusterOptions())
# 
# leaflet::leaflet(data = low_speed_map, options = leaflet::leafletOptions(preferCanvas = TRUE)) %>% 
#   leaflet::addTiles(options = leaflet::tileOptions(updateWhenZooming = FALSE,
#                                                          updateWhenIdle = TRUE)) %>% 
#   leaflet::addMarkers(lng = ~longitude.x, lat = ~latitude.x)


```

Vessels travelling already at around 7-10knots will (or even probably slightly faster) will not slow down for whales unless absolutely necessary. Personal experience and anecdotal evidence suggests that it does happen, particularly closer to port, but it isn't likely. I suggest filtering data \<= 10knots and \> 40 knots.

Filter out all of the data under 0.2 knots + over 40.2 knots. Group by class and calculate difference to median for speed of vessels.

```{r}

# clean_wras_ais_filtered = matched_wras_ais %>%
#   dplyr::filter(sog > 10 & sog < 40) %>%
#   dplyr::mutate(type = dplyr::case_when(stringr::str_detect(type, "Tanker") ~ "Tanker",
#                                         stringr::str_detect(type, "Cargo ship") ~ "Cargo ship",
#                                         stringr::str_detect(type, "Other") ~ "Other",
#                                         TRUE ~ type))

# clean_wras_ais_non_filtered = matched_wras_ais %>%
#   dplyr::filter(sog < 40.2) %>% 
#   dplyr::mutate(type = dplyr::case_when(stringr::str_detect(type, "Tanker") ~ "Tanker",
#                                         stringr::str_detect(type, "Cargo ship") ~ "Cargo ship",
#                                         stringr::str_detect(type, "Other") ~ "Other",
#                                         TRUE ~ type))

```

Is there some groups that just don't need to be included as they are so small?

data_count = clean_wras_ais_filtered %\>%

dplyr::group_by(type) %\>%

dplyr::summarise(count = dplyr::n_distinct(alert_sent))

```{r}
# data_count = clean_wras_ais_filtered %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(count = dplyr::n_distinct(alert_sent))

```

Filter out everything which is under 100 sample size, leaving us with..

```{r}
# vessel_type_filter = data_count %>% dplyr::filter(count > 100) %>% .$type
# vessel_type_filter
```

Left with "Passenger Ship", "HSC" (high speed craft), and "Cargo ship", totaling 3394 alerts.

#### Takeaways

-   We need to filter the data by vessels \> 7 knots and \< 40 knots

-   We will filter the groups of vessel types which have over 100 records, this includes HSC, Passenger ships, and Cargo Ships

## Streamlining baseline data area

-   I want to see where vessels are travelling when they recieved alerts, do patters (dependant of vessel type) emerge, can I use those to fine tune the study?

```{r}
# baseline_data %>%
#   dplyr::group_by(date, mmsi) %>%
#   dplyr::summarise(latitude = mean(latitude), longitude = mean(longitude)) %>%
#   sf::st_as_sf(coords = c("longitude", "latitude")) %>%
#   leaflet::leaflet() %>%
#   leaflet::addTiles() %>%
#   leaflet::addCircleMarkers(radius = 0.5) %>%
#   leaflet::addCircleMarkers(data = for_analysis,
#                              lat = for_analysis$latitude.x,
#                              lng = for_analysis$longitude.x,
#                              radius = 0.5, col = "red")


```

-   The above shows that the baseline data has a much wider extent than the analysis data. I need to take area out as a variable in the speed study. In order to do this I will set an area for the study with a custom polygon and look within this range instead.
    -   For now I am just using the Salish Sea (ish) as a polygon, but I will expand this to be Seattle...

```{r}
# data_filter_shp = sf::st_read("../../../../Ocean Wise Conservation Association/Project - WRAS - Efficacy/Resources/efficacy-area-filter.shp") %>% 
#   sf::st_set_crs(4326)

data_filter_shp = sf::st_read("shapefiles/processed/efficacy-area-filter/efficacy-area-filter.shp") %>% 
  sf::st_set_crs(4326) 
# 
# leaflet::leaflet() %>%
#   leaflet::addTiles() %>%
#   leaflet::addPolygons(data = data_filter_shp)

# baseline_data %>%
#   dplyr::group_by(date, mmsi) %>% 
#   dplyr::summarise(latitude = mean(latitude), longitude = mean(longitude)) %>% 
#   sf::st_as_sf(coords = c("longitude", "latitude")) %>%
#   leaflet::leaflet() %>% 
#   leaflet::addTiles() %>% 
#   leaflet::addCircleMarkers(radius = 0.5) %>% 
#   leaflet::addPolygons(data = data_filter_shp)

# 
# ## Define chunk size
# chunk_size <- 10000
# 
# # Split your data into chunks
# num_chunks = ceiling(nrow(baseline_data) / chunk_size)
# chunks <- split(baseline_data, rep(1:num_chunks, each = chunk_size, length.out = nrow(baseline_data)))
# 
# ## Define a function for spatial join
# perform_spatial_join <- function(chunk) {
# 
#   chunk_sf <- sf::st_as_sf(chunk, coords = c("longitude", "latitude"), crs = sf::st_crs(data_filter_shp))
# 
#   ## Spatial join using chunk's spatial index
#   # joined_data <- sf::st_join(chunk_sf, data_filter_shp, join = sf::st_within) # left = FALSE, right = FALSE, use_sindex = TRUE)
#   joined_data = chunk_sf%>%
#     dplyr::mutate(intersection = as.integer(sf::st_intersects(geometry, data_filter_shp))) %>%
#     dplyr::filter(is.na(intersection)==F)
# 
#   return(joined_data)
# }
# 
# 
# 
# filtered_baseline_data = purrr::map(chunks, perform_spatial_join) %>%
#   dplyr::bind_rows()

filtered_analysis_data = sf::st_as_sf(for_analysis, 
                                      coords = c("longitude.y", "latitude.y"), 
                                      crs = sf::st_crs(data_filter_shp)) %>% 
  dplyr::mutate(intersection = as.integer(sf::st_intersects(geometry, data_filter_shp))) %>% 
  dplyr::filter(is.na(intersection)==F)

  # rm(chunks)
# 
filtered_analysis_data %>%
  leaflet::leaflet() %>%
  leaflet::addTiles() %>%
  leaflet::addCircleMarkers(radius = 0.5) %>% 
  leaflet::addPolygons(data = data_filter_shp)



 
```

-   Now i've matched the WRAS data and the baseline data, I can repeat my speed analysis and see if there is a difference.
    -   WRAS data is around 50% reduced, baseline data is around 83% reduced

```{r}

# median_filtered_wras_data = filtered_analysis_data %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(median_test = median(sog)) %>% 
#   sf::st_drop_geometry()
# 
# median_filtered_baseline_data = filtered_baseline_data %>% 
#   dplyr::filter(type %in% median_filtered_wras_data$type) %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(median_baseline = median(sog)) %>%
#   sf::st_drop_geometry()
# 
# baseline_vs_wras = filtered_analysis_data %>% 
#   dplyr::filter(type %in% median_filtered_wras_data$type) %>% 
#   dplyr::left_join(median_filtered_baseline_data, by = "type") %>% 
#   dplyr::left_join(median_filtered_wras_data, by = "type") %>% 
#   dplyr::group_by(type) %>%
#   ggplot2::ggplot(ggplot2::aes(x = sog)) + 
#   ggplot2::geom_histogram(binwidth = 1) + 
#   # ggplot2::geom_histogram(data = filtered_analysis_data, binwidth = 1, color = "purple") +
#   ggplot2::facet_wrap(~type, scales = "free") +
#   ggplot2::geom_vline(ggplot2::aes(xintercept = median_baseline), color = "red", linetype = "dashed") + 
#   ggplot2::geom_vline(ggplot2::aes(xintercept = median_test), color = "darkgrey", linetype = "solid") + 
#   ggplot2::labs(x = "SOG", y = "Frequency") + 
#   ggplot2::theme_minimal()
# 
# baseline_vs_wras
# 
# rm(baseline_plot)
# rm(baseline_vs_wras)

```

## Types of vessels...

-   I've noticed that in the AIS data there are many sub-categories of vessels found within "Passenger ship", predominantly small eco-tourism vessels. These vessels aren't covered under WRAS access, so in order to match the baseline data to vessels within WRAS I will have to come up with a way of filtering out these vessels from the baseline data.

-   My first thought is to look at length to see if there is something I can do there...

    -   I will make plots of the distribution of length from baseline data and WRAS data and see what comes out.

```{r}
# library(patchwork)  
# 
# baseline_plot = filtered_baseline_data %>% 
#   dplyr::filter(type %in% unique(filtered_analysis_data$type)) %>% 
#   ggplot2::ggplot(data = ., ggplot2::aes(x = type, y = length, fill = type)) +   
#   ggplot2::geom_boxplot(show.legend = F) +   
#   ggplot2::theme_minimal() +
#   ggplot2::coord_cartesian(ylim = c(0, 520)) +
#   ggplot2::ggtitle("Baseline")
# 
# analysis_plot = filtered_analysis_data %>%    
#   ggplot2::ggplot(data = ., ggplot2::aes(x = type, y = length, fill = type)) +   
#   ggplot2::geom_boxplot(show.legend = F) +   
#   ggplot2::theme_minimal() +
#   ggplot2::coord_cartesian(ylim = c(0, 520)) + 
#   ggplot2::ggtitle("WRAS")
# 
# combined_plots = baseline_plot + analysis_plot
# 
# combined_plots
# 
# rm(combined_plots)
# rm(baseline_plot)
# rm(analysis_plot)
```

## 

-   Looking at the above plot we can see that the distribution of lengths for cargo and passenger ships is different between the baseline and the analysis data. This suggests a difference in vessel types within these broad categories which could mean a difference in speeds. To check this I'll look at a scatter plot.

    -   I'm more focused on the passenger vessels rather than the cargo ships. If I remove outliers and match the min vessel length for the baseline to the min length of the WRAS data we will have a pretty good standardization for this group.

    -   Passenger vessels is a bit trickier in my opinion so I will try to better understand the data before filtering.

```{r}
# filtered_baseline_data %>%
#   dplyr::filter(type %in% unique(filtered_analysis_data$type)) %>% 
#   dplyr::group_by(type) %>% 
#   ggpubr::ggscatter(., x = "sog", y = "length", col = "type")
  

  
```

-   Looks like my prediction was correct. In Passenger vessels, smaller vessels travel faster than larger vessels.

    -   I should filter this passenger data down to match the analysis data.

-   I'm going to run a quick plot of the WRAS dataset to see the differences...

```{r}
filtered_analysis_data %>% 
  dplyr::filter(type %in% unique(filtered_analysis_data$type)) %>% 
  dplyr::group_by(type) %>% 
  ggpubr::ggscatter(., x = "sog", y = "length", col = "type")
```

-   We can see by the above scatter plots that there is a distinct weighting of baseline data towards smaller, faster vessels for passenger type vessels (and to a lesser extent but still present in cargo vessels).

-   I am removing outliers via the **interquartile range method**

-   To dos...

    -   [x] Compare distribution of speed vs length

    -   [x] Clean the cargo ship data to remove outliers and smaller vessels than the min of the test group.

    -   [x] Clean passenger category - smaller vessels to be removed (based on scatter plot)

    -   [x] re run analysis

### Removing outliers and filtering baseline data

```{r}

## Remove outliers 
clean_analysis_data = filtered_analysis_data %>%
  dplyr::filter(!(type == "Passenger ship" & length < 70 & length > 200))


## Set the limits of the baseline data to the analysis data 
# clean_baseline_data = filtered_baseline_data %>% 
#   dplyr::filter(length <= max(clean_analysis_data$length) &
#                   length >= min(clean_analysis_data$length),
#                 name %in% clean_analysis_data$name)
  
clean_analysis_data %>%
  leaflet::leaflet() %>%
  leaflet::addTiles() %>%
  leaflet::addCircleMarkers(radius = 0.5)

```

-   Not that I am hunting for answers, but I would like to see whether there is another factor impacting the vessel speed for the passenger vessels.

    -   is the WRAS data including High Speed Ferries in here when the Baseline is not?

    -   Is there a major difference in routes that are being compared with the WRAS and Baseline?

### Vessel type exploration

Can I match vessel names? Are they the same kind of vessels?

```{r}

# vessels_analysis = clean_analysis_data %>% 
#   dplyr::filter(type == "Passenger ship")
# 
# unique(vessels_analysis$name)
# 
# vessel_matched_baseline = clean_baseline_data %>% 
#   dplyr::filter(type == "Passenger ship" &
#                 name %in% unique(vessels_analysis$name))

# vessel_matched_baseline %>%
#   leaflet::leaflet() %>%
#   leaflet::addTiles() %>%
#   leaflet::addCircleMarkers(radius = 0.5) %>%
#   leaflet::addCircleMarkers(data = vessels_analysis, radius = 0.5, col = "red")




```

Looking at the above we can see that there are many vessels in the baseline that don't appear in the WRAS data. In the chunk above I have made sure to match vessel names in the WRAS data with those in the baseline.

# Analysis 1.0 - Speed

## Speed

```{r}
# median_filtered_wras_data = clean_analysis_data %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(median_test = median(sog)) %>% 
#   sf::st_drop_geometry()
# 
# median_filtered_baseline_data = clean_baseline_data %>% 
#   dplyr::filter(type %in% median_filtered_wras_data$type) %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(median_baseline = median(sog)) %>%
#   sf::st_drop_geometry()
# 
# x = clean_baseline_data %>% 
#   dplyr::filter(type %in% median_filtered_wras_data$type) %>%
#   # dplyr::filter(type == "Cargo ship") %>% 
#   dplyr::left_join(median_filtered_baseline_data, by = "type") %>% 
#   dplyr::left_join(median_filtered_wras_data, by = "type") %>% 
#   dplyr::group_by(type) %>%
#   ggplot2::ggplot(ggplot2::aes(x = sog)) + 
#   ggplot2::geom_histogram(binwidth = 1) + 
#   ggplot2::facet_wrap(~type, scales = "free") +
#   ggplot2::geom_vline(ggplot2::aes(xintercept = median_baseline), color = "darkgrey", linetype = "solid",linewidth = 1.25) + 
#   ggplot2::geom_vline(ggplot2::aes(xintercept = median_test), color = "orange", linetype = "dashed", linewidth = 1.25) + 
#   # ggplot2::labs(x = "SOG", y = "Frequency") + 
#   ggplot2::theme_minimal() +
#   ggplot2::theme(
#     panel.grid.major.x = ggplot2::element_blank(),
#     panel.grid.minor.x = ggplot2::element_blank(),
#     axis.text = ggplot2::element_text(size = 16)
#   )
# 
# x
# 
# ggplot2::ggsave(filename = "/Users/AlexMitchell/Downloads/efficacy-result.jpg", plot = x)
```

## Normality Test

Is this difference we are seeing significant?

```{r}

# ## Cargo ship 
# cargo_analy = clean_analysis_data %>%
#   dplyr::filter(type == "Cargo ship")
# 
# cargo_base = clean_baseline_data %>%
#   dplyr::filter(type == "Cargo ship")
# 
# qqnorm(cargo_analy$sog)
# 
# ## both normal == t.test
# 
# t.test(cargo_base$sog, cargo_analy$sog)
# 
# ## Passenger ship
# pass_analy = clean_analysis_data %>%
#   dplyr::filter(type == "Passenger ship")
# 
# pass_base = clean_baseline_data %>%
#   dplyr::filter(type == "Passenger ship")
# 
# qqnorm(pass_base$sog)
# 
# ## not normal == wilcox.test
# 
# wilcox.test(pass_base$sog, pass_analy$sog)
```

## Results

1.  Cargo ships median speed is 4 knots slower when using WRAS compared to those who don't.

    1.  This difference is statistically significant

2.  Passenger vessels show an increase in speed when using WRAS, which is an unexpected result.

    1.  This difference is also statistically significant but doesn't align with anecdotal and other evidence

### Next steps

-   Look at the difference before an after receiving and alert

-   Test whether there is a difference in speed response if a sighting is in front of the vessel or not

# Analysis 1.1 - Speed

-   I need to have a way of creating a before and after interval from an alert period

-   I then need to label whether the vessel received the alert before or after

    -   This is when I would then look at the sightings and tag whether the sighting was before or after the sighting

    -   maybe not actually analyze this... but just look at whether it is or not...

-   Then I can look at before and after speeds of vessels and see whats going on.

-   So.

    1.  Create interval of 30mins before + 30mins after (I probably need to map this data and see where the vessels are before and after... if I don't have data for the before period as they're in harbor it'll show a increase in speed
    2.  Identify whether the whale was within 120 degrees when received the alert.
    3.  Tag the direction of whale
    4.  speed analysis before and after alert

Need to think about the data structure, might be a couple of datasets to produce.

## WRAS data

```{r warning = F, message = F}
# user_filter = users_clean$name[users_clean$include_in_efficacy_analysis == "no"]
# 
# WRAS_data_cleaned = WRAS_data_midclean %>% 
#   dplyr::filter(!skipper %in% user_filter) %>% 
#   dplyr::select(-c(key1, key2, notes, tier)) %>% 
#   dplyr::mutate(mmsi = as.numeric(mmsi)) %>% 
#   dplyr::mutate(range = lubridate::interval(alert_sent - lubridate::minutes(30),# create an interval 30mins before and after an alert was sent.
#                                             # alert_sent, 
#                                             alert_sent + lubridate::minutes(30)),
#                 date = lubridate::as_date(alert_sent)) 
# 
# ## Remove uneccesary objects from environment to reduce storage in memory.
# rm(WRAS_data_matched, WRAS_data_midclean)
```

## AIS data reload

I can just rerun the previous section above to load the AIS data, but I will not produce any baseline data as the data itself is the baseline as I am using **before** vs **after** alert.

@ais-data

No that we have our have created a list of files from within the folders of AIS data between 2019 - 2022 we can loop over these file names and join to our WRAS data creating our final dataset.

## Data cleaning

Cleaning has been done whilst loading, the next step is to filter for the area of data that I am interested in. I can do that by running code found in @streamlining-baseline-data-area

Now I need to label whether the event has any data from before the alert was sent, if not I need to remove that group. This can be done two ways:

-   label whether the group has data before alert, if no then remove, then label before and after.

-   label before and after, then count the amount of before in the group, if this number is 1 or less then remove.

I think #2 would be best here as it'll give me a quicker answer.

```{r}

speed_clean_2 = clean_analysis_data %>%
  # get distinct values from the dataset, there are some repeats of the AIS data. 3 rows for the same time.
  dplyr::distinct() %>%
  # create groups of vessels and alerts
  dplyr::group_by(mmsi, alert_sent, species) %>%
  # label data for before or after the alert was sent
  dplyr::mutate(before_after =
                  dplyr::case_when(local_time <= alert_sent ~ "before",
                                   local_time > alert_sent ~ "after")) %>%
  # count the befores and afters, I want a minimum of 3 rows of data. Might be
  dplyr::mutate(after_count = sum(before_after == "after"),
                before_count = sum(before_after == "before")) %>%
  # filter out all data less than this
  dplyr::filter(after_count >= 3 & before_count >= 3) %>%
  dplyr::select(-c(match, after_count, before_count)) %>%
  dplyr::group_by(before_after, .add = T) %>%
  dplyr::mutate(mean_speed = mean(sog))

# speed_clean_2 %>%
#   leaflet::leaflet() %>%
#   leaflet::addTiles() %>%
#   leaflet::addCircleMarkers(radius = 0.5)
## Check the spatial area, make sure the points are in the correct area


## Create dataset with speed comparrisons
speed_2 = speed_clean_2 %>%
  sf::st_drop_geometry() %>%
  dplyr::select(c(mmsi,
                  name,
                  type,
                  class,
                  length,
                  species,
                  alert_sent,
                  before_after,
                  mean_speed)) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(mmsi, alert_sent) %>%
  dplyr::distinct() %>%
  tidyr::pivot_wider(names_from = before_after,
                     values_from = mean_speed) %>%
  dplyr::mutate(
    speed_diff = before - after
  )

## We have 1632 unique records in here.
## 248 cargo ships, 1285 Passenger ships, 99 HSC (fast ferries)
speed_2 %>%
  dplyr::group_by(type) %>%
  dplyr::summarise(n = dplyr::n())

## Violin plot to show frequency of speed differences after recieving an alert
speed_2 %>%
  plotly::plot_ly(x = ~type,
                  y = ~speed_diff,
                  type = 'violin',
                  box = list(visible = T),
                  meanline = list(visible = T))

```

### Comments

This shows that, for all 3 groups there is a relatively even distribution of vessels which were faster or slower after receiving an alert.

It isn't clear here why that might be, and I think a difference of 1-2knots isn't significant enough to be worth looking at, so, I think I will filter out data from 0-2knots either side and see what the distribution is like.

I now need to see whether there is a difference of sighting in front of vessel.

```{r}

## How many records does this leave?
# speed_2 %>% 
#   dplyr::filter(speed_diff < -2 | speed_diff > 2) %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(n = dplyr::n())
# ## 41 Cargo, 58 HSC, 440 Passenger
# 
# ## Yes vs No
# speed_2 %>% 
#   dplyr::filter(speed_diff < -2 | speed_diff > 2) %>% 
#   dplyr::mutate(
#     speed_change = 
#       dplyr::case_when(
#         speed_diff < 0 ~ "yes",
#         speed_diff > 0 ~ "no")) %>% 
#   dplyr::ungroup() %>% 
#   dplyr::group_by(
#     type,
#     speed_change
#   ) %>% 
#   dplyr::summarise(n = dplyr::n()) %>% 
#   tidyr::pivot_wider(
#     names_from = speed_change,
#     values_from = n
#   ) %>% 
#   plotly::plot_ly(x = ~type, y = ~yes, type = "bar", name = "Yes", color = "#3f9346") %>% 
#   plotly::add_trace(y = ~no, name = "No", color = "#AA4A44") %>% 
#   plotly::layout(yaxis = list(title = 'Count'), 
#                  barmode = 'group')
  


```

```{r}

## Violin plot to see distrubtion
speed_2 %>% 
  dplyr::filter(speed_diff < -2 | speed_diff > 2) %>% 
    plotly::plot_ly(x = ~type, 
                  y = ~speed_diff,
                  type = 'violin',
                  box = list(visible = T),
                  meanline = list(visible = T)) 
  
```

## 

### Where are these vessel groups slowing down?

```{r}

## Take speed_2 and lets map out where these different vessels are slowing down and add colours to differentiate the different types. 

# vessel_map_filter = speed_2 %>% 
#   dplyr::select(mmsi, alert_sent, species, speed_diff)
# 
# speed_clean_2 %>%
#   sf::st_drop_geometry() %>% 
#   leaflet::leaflet() %>%
#   leaflet::addTiles() %>%
#   leaflet::addCircleMarkers(~longitude.x,
#                             ~latitude.x,
#                             radius = 0.5)
# 
# speed_clean_2 %>% 
#   dplyr::left_join(vessel_map_filter, by = c("mmsi", "alert_sent", "species")) %>% 
#   dplyr::filter(speed_diff < 0) %>%
#   dplyr::mutate(
#     color_col = dplyr::case_when(
#       type == "Cargo ship" ~ "blue",
#       type == "Passenger ship" ~ "red",
#       type == "HSC" ~ "green"
#     )
#   ) %>%
#   leaflet::leaflet() %>% 
#   leaflet::addTiles() %>% 
#   leaflet::addCircleMarkers(
#     ~longitude.x,
#     ~latitude.x,
#     popup = ~speed_diff,
#     label = ~speed_diff,
#     radius = 0.5,
#     color = ~color_col
#   )


```

## Results

This shows that, for all 3 groups there is a relatively even distribution of vessels which were faster or slower after receiving an alert.

It isn't clear here why that might be, and I think a difference of 1-2knots isn't significant enough to be worth looking at, so, I think I will filter out data from 0-2knots either side and see what the distribution is like.

I now need to see whether there is a difference of sighting in front of vessel.

**TRIGONOMETRY TIME - WHERE IS THE WHALE IN RELATION TO THE VESSEL?**

# Analysis 1.2 - Speed + is the whale in front?

First step is to write a function which can be used to detect a whale in front of a vessel, or within 120 degree of the vessel?

```{r}

## We removed the lat lon of the sighting earlier to create a geometry column, we need to get those back. 

sighting_coords = unlist(sf::st_geometry(speed_clean_2$geometry)) %>% 
  matrix(ncol = 2, byrow = T) %>% 
  tibble::as_tibble() %>% 
  setNames(c("sighting_longitude", "sighting_latitude"))

## Now we should tidy up our dataset a little bit

speed_3 = speed_clean_2 %>% 
  # dplyr::select(c(
  #   local_time,
  #   mmsi,
  #   name,
  #   type,
  #   length,
  #   sog,
  #   heading,
  #   vessel_latitude = latitude.x,
  #   vessel_longitude = longitude.x,
  #   alert_sent,
  #   species,
  #   vessel,
  #   vessel_name,
  #   before_after,
  #   geometry
  # )) %>% 
  dplyr::rename(c(vessel_latitude = latitude.x,
                  vessel_longitude = longitude.x)) %>% 
  dplyr::bind_cols(., sighting_coords) %>% 
  sf::st_drop_geometry()
```

Now I need to create a function to calculate whether the whale is in-front of the vessel or not...

```{r}
# ## bearing between two points
# calc_bearing =  function(lat1,lat2,lon1,lon2){
#   bearing = geosphere::bearingRhumb(c(lon1, lat1), c(lon2, lat2))
#   return(bearing)
# }

rm(list = c("speed_2", "speed_clean_2"))

## Run the function on the data... 
# (bearingRhumb(c(vessel_lon, vessel_lat), c(sighting_lon, sighting_lat)) + 360) %% 36

speed_clean_3 = speed_3 %>% 
  dplyr::rowwise() %>% 
  dplyr::mutate(
    bearing_to_whale = (
      geosphere::bearingRhumb(
        c(vessel_longitude,vessel_latitude),
        c(sighting_longitude,sighting_latitude)) + 360) %% 360,
    vessel_heading = (heading + 360) %% 360,
    ## 120 degrees as the vessel has watchers either side of the vessel meaning the field of view is larger than 120.
    in_view = abs(bearing_to_whale - vessel_heading) <= 60 | 
              abs(bearing_to_whale - vessel_heading) >= 300
    ) %>% 
  dplyr::ungroup()

## check the field of view filter and how many that leaves
speed_clean_3 %>% 
  dplyr::group_by(in_view) %>% 
  dplyr::summarise(n = dplyr::n())




```

**Is there a way we can filter out people that receive their own alert?**

I'm actually going to ignore the above, as if they submitted a detection we would hope that they would also slow down...

So... lets filter the data by grouping the data into vessel name, alert sent - and if there was a point at which the vessel was in view of the location of the sighting before receiving an alert, did it slow down...

I also thought about the following - *How many vessels have a sighting in front of them within 10mins of receiving an alert? -* I'm also going to discount this. I think that at some point if the alert is in their general direction then it's more likely they'll take action.

```{r}

in_front_analysis = speed_clean_3 %>% 
  dplyr::group_by(alert_sent, vessel) %>% 
  dplyr::filter(any(in_view == T)) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(c(mmsi,
                  name,
                  type,
                  length,
                  species,
                  alert_sent,
                  before_after,
                  mean_speed)) %>%
  dplyr::group_by(mmsi, alert_sent) %>%
  dplyr::distinct() %>%
  tidyr::pivot_wider(names_from = before_after,
                     values_from = mean_speed) %>%
  dplyr::mutate(
    speed_diff = before - after
  )
  



```

#### What is the distribution of vessel speed changes per type

```{r}

in_front_analysis %>% 
  dplyr::ungroup() %>% 
  dplyr::group_by(type) %>% 
  dplyr::mutate(speed_diff = 
                  dplyr::case_when(speed_diff < 0 ~ floor(speed_diff),
                                 speed_diff > 0 ~ ceiling(speed_diff))) %>% 
  dplyr::filter(speed_diff < -2 | speed_diff > 2) %>%
  dplyr::select(c(type, speed_diff)) %>% 
  split(.$type) %>% 
  lapply(function(df) {
    plotly::plot_ly(df,
                    x = ~speed_diff,
                    type = "histogram",
                    name = unique(df$type))
  }) %>% 
  plotly::subplot(.,
                  nrows = 3,
                  shareX = T,
                  titleX = T) %>% 
  plotly::layout(title = "",
                 xaxis = list(title = "speed_diff"),
                 yaxis = list(title = "count"))
  

```

#### Analysis of vessel speed when sighting is (at some point) in front of vessel

```{r}

speed_clean_3 %>% 
  dplyr::group_by(alert_sent, type, vessel) %>% 
  dplyr::filter(any(in_view == T)) %>% 
  # only want to see speed after recieving alert
  dplyr::filter(before_after == "after") %>% 
  dplyr::select(alert_sent, type, mean_speed) %>% 
  dplyr::distinct() %>% 
  dplyr::ungroup() %>% 
  dplyr::group_by(type) %>% 
  dplyr::select(c(type, mean_speed)) %>% 
  split(.$type) %>% 
  lapply(function(df) {
    plotly::plot_ly(df,
                    x = ~mean_speed,
                    type = "histogram",
                    name = unique(df$type))
  }) %>% 
  plotly::subplot(.,
                  nrows = 3,
                  shareX = T,
                  titleX = T) %>% 
  plotly::layout(title = "",
                 xaxis = list(title = "mean_speed"),
                 yaxis = list(title = "count"))
  
  
```

```{r}
speed_clean_3 %>% 
  dplyr::group_by(alert_sent, type, vessel) %>% 
  dplyr::filter(any(in_view == T)) %>% 
  # only want to see speed after recieving alert
  dplyr::filter(before_after == "before") %>% 
  dplyr::select(alert_sent, type, mean_speed) %>% 
  dplyr::distinct() %>% 
  dplyr::ungroup() %>% 
  dplyr::group_by(type) %>% 
  dplyr::select(c(type, mean_speed)) %>% 
  split(.$type) %>% 
  lapply(function(df) {
    plotly::plot_ly(df,
                    x = ~mean_speed,
                    type = "histogram",
                    name = unique(df$type))
  }) %>% 
  plotly::subplot(.,
                  nrows = 3,
                  shareX = T,
                  titleX = T) %>% 
  plotly::layout(title = "",
                 xaxis = list(title = "mean_speed"),
                 yaxis = list(title = "count"))
  
  
```

Removing the speed differences of +/- 2 knots shows that HSC and Passenger ships have distinct groups which slow, and, shows the mean and median lower than 0. So, on average, vessels slow down. This to me feels like a stretch however.

But what speed are they actually going? Not the speed diff but what speed are they going?

# To Do....

-   [ ] I want to rerun this analysis with more data (waiting for CCG).

-   [ ] What about proximity?

-   [ ] What speed are vessels going when within X distance of whales.

# Sandbox

### Baseline Data

```{r eval=FALSE}
  

x = in_front_analysis %>% 
  dplyr::filter(type == "HSC")
  
unique(x$name)




# median_baseline_filtered = baseline_data %>% 
#   # dplyr::filter(sog > 7) %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(median = median(sog))
# 
# median_baseline_non_filtered = baseline_data %>% 
#   dplyr::group_by(type) %>% 
#   dplyr::summarise(median = median(sog))
# 
# 
# baseline_plots_no_lower = baseline_data %>% 
#   dplyr::filter(sog > 7) %>% 
#   dplyr::left_join(median_baseline_filtered, by = "type") %>% 
#   dplyr::group_by(type) %>%  
#   ggplot2::ggplot(ggplot2::aes(x = sog)) +
#   ggplot2::geom_histogram(binwidth = 1) +
#   ggplot2::geom_vline(ggplot2::aes(xintercept = median), color = "red", linetype = "dashed") +
#   ggplot2::facet_wrap(~type, scales = "free") +
#   ggplot2::labs(x = "SOG", y = "Frequency") +
#   ggplot2::theme_minimal()
# 
# baseline_plots_lower = baseline_data %>% 
#   dplyr::left_join(median_baseline_non_filtered, by = "type") %>% 
#   dplyr::group_by(type) %>%  
#   ggplot2::ggplot(ggplot2::aes(x = sog)) +
#   ggplot2::geom_histogram(binwidth = 1) +
#   ggplot2::geom_vline(ggplot2::aes(xintercept = median), color = "red", linetype = "dashed") +
#   ggplot2::facet_wrap(~type, scales = "free") +
#   ggplot2::labs(x = "SOG", y = "Frequency") +
#   ggplot2::theme_minimal()
# 
# ggpubr::ggarrange(baseline_plots_no_lower, baseline_plots_lower,
#                   labels = c("filtered", "non-filtered"),
#                   nrow = 2)


# clean_baseline_data %>%
#   leaflet::leaflet() %>%
#   leaflet::addTiles() %>%
#   leaflet::addCircleMarkers(radius = 0.5) %>%
#   leaflet::addCircleMarkers(data = clean_analysis_data, radius = 0.5, col = "red")
# 
# 
# 
speed_clean_3 %>%
  dplyr::filter(type == "HSC") %>% 
  leaflet::leaflet() %>%
  leaflet::addTiles() %>%
  leaflet::addCircleMarkers(lat = ~vessel_latitude, lng = ~vessel_longitude
                            , radius = 0.5, col = "purple") %>% 
  leaflet::addPolygons(data = data_filter_shp)

```

I want to quickly map the "infront of view" for a couple of sightings and see whats going on on a map...

```{r}
# example_infront = speed_clean_3 %>% 
#   # dplyr::filter() %>% 
#   head(11) %>% 
#   write.csv(., file = "../../../../Downloads/speed_test_qgis.csv")



```
